{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During EDA we found out that, with regards to statistical measures in this exercise, NBA centers from the US and EU are essentially the same. Put another way, the statistical measures for (which we will build our predictive models) are independant of the 'US or EU' column. \n",
    "\n",
    "In this notebook, we will build predictive models for: Points, Assists, Rebounds, Steals and Blocks. Given that this is probably the most useful part of the project in terms of delivering real world value, the descriptions and comments/justifications are less technical and shorter to not dilute the the results of the models and other findings. Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas Buteliauskas\\Anaconda3_\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, normalize\n",
    "from sklearn.feature_selection import RFECV\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS \n",
    "from scipy.stats import boxcox\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "plt.rcParams[\"figure.figsize\"] = [18,10]\n",
    "\n",
    "with open(\"C:/Users/Lukas Buteliauskas/Desktop/Springboard Projects/Capstone Project 1 - NBA Analytics/\"\n",
    "          \"3. Exploratory Data Analysis/Player Data Filtered.csv\", \"r\") as player_data_file:\n",
    "    player_data_df=pd.read_csv(player_data_file)\n",
    "\n",
    "player_data_df_undrafted=player_data_df[player_data_df[\"Draft Placing\"].isnull()].reset_index(drop=True) #undrafted player data\n",
    "player_data_df=player_data_df[player_data_df[\"Draft Placing\"].isnull()==False].reset_index(drop=True) # drafted player data\n",
    "\n",
    "player_data_og=player_data_df # keeping a copy of the input data (for drafted players)\n",
    "player_data_df=player_data_df.loc[(player_data_df[\"eFG%\"]!=0.0) & (player_data_df[\"eFG%\"]!=100.0)] # filtering out 0 or 100 eFG%\n",
    "player_data_df=player_data_df.iloc[:,[0,1,2,3,4,5,6,7,8,9,10,11,13]].reset_index(drop=True) #dropping US or EU\n",
    "\n",
    "# Dropping the 'US or EU' and 'Draft Placing' columns for undrafted player data.\n",
    "player_data_df_undrafted=player_data_df_undrafted.iloc[:,[0,1,2,3,4,6,7,8,9,10,11,13]].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that undrafted players make up a sizeable amount of data, and are themselves a specific group of players, their data cannot be disregarded, but at the same time the Draft Rank cannot be reverse engineered. Hence, it makes sense to consider both groups as distinct, and attempt to build models for both seperately. We will begin with models for drafted players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3860 entries, 0 to 3859\n",
      "Data columns (total 13 columns):\n",
      "2PA              3860 non-null float64\n",
      "3P               3860 non-null float64\n",
      "AST              3860 non-null float64\n",
      "Age              3860 non-null int64\n",
      "BLK              3860 non-null float64\n",
      "Draft Placing    3860 non-null float64\n",
      "MP               3860 non-null float64\n",
      "PF               3860 non-null float64\n",
      "PTS              3860 non-null float64\n",
      "STL              3860 non-null float64\n",
      "TOV              3860 non-null float64\n",
      "TRB              3860 non-null float64\n",
      "eFG%             3860 non-null float64\n",
      "dtypes: float64(12), int64(1)\n",
      "memory usage: 392.1 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 471 entries, 0 to 470\n",
      "Data columns (total 12 columns):\n",
      "2PA     471 non-null float64\n",
      "3P      471 non-null float64\n",
      "AST     471 non-null float64\n",
      "Age     471 non-null int64\n",
      "BLK     471 non-null float64\n",
      "MP      471 non-null float64\n",
      "PF      471 non-null float64\n",
      "PTS     471 non-null float64\n",
      "STL     471 non-null float64\n",
      "TOV     471 non-null float64\n",
      "TRB     471 non-null float64\n",
      "eFG%    471 non-null float64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 44.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(player_data_df.info())\n",
    "print(player_data_df_undrafted.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat_selector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dc6018f7b9da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;34m\"\"\" For real world applications, checking against new data.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeat_selector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpol_degree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#if 'data' is a dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPolynomialFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpol_degree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feat_selector' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" This prints the R^2 statistic for training and testing, the cross validated R^2 mean, 95% CI as well as the MSE.\n",
    "We also print 'n_test_print' rows showing the actual value, the value predicted by the model and the absolute difference. \"\"\"\n",
    "def performance_prints(clf, X_train, X_test, y_train, y_test, cv=5, n_test_print=15):\n",
    "    print(\"Training performance (R^2): %.3f\" % clf.score(X_train, y_train))\n",
    "    print(\"Testing performance (R^2): %.3f\" % clf.score(X_test, y_test))\n",
    "    scores=cross_val_score(clf, X_train, y_train, cv=cv)\n",
    "    print(\"R^2 cross val mean: %.3f\" % (scores.mean()))\n",
    "    print(\"95%% confidence interval for R^2: (%.3f, %.3f)\" % (scores.mean()-scores.std()*2, scores.mean()+scores.std()*2))\n",
    "    print(\"Mean Squared Error (MSE): %.3f\\n\\nSome test prints:\" % mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    for idx, data in enumerate(zip(y_test, y_pred)): \n",
    "        if idx<n_test_print:\n",
    "            print(\"Actual: %.2f\\t  Predicted: %.2f\\tDifference (absolute): %.3f\" % (data[0], data[1], np.abs(data[0]-data[1])))\n",
    "            \n",
    "\"\"\" Residual Plot. Predicted values (y_pred) on the X-axis, residuals on the Y-axis. \"\"\"\n",
    "def my_resid_plot(y_test, y_pred, label_name=\"\", standardized=False): # for showing us the residual plots\n",
    "    residuals=y_test-y_pred\n",
    "    if standardized is True:\n",
    "        residuals/=np.std(residuals)\n",
    "        plt.scatter(y_pred, residuals, color=\"black\")\n",
    "        plt.title(\"Residual Plot\")\n",
    "        plt.xlabel(\"Predicted \" + label_name + \" Values\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.axhline(y=0, linewidth=3, color=\"black\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.scatter(y_pred, residuals, color=\"black\")\n",
    "        plt.title(\"Residual Plot\")\n",
    "        plt.xlabel(\"Predicted \" + label_name + \" Values\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.axhline(y=0, linewidth=3, color=\"black\")\n",
    "        plt.show()\n",
    "\n",
    "\"\"\" Residual plots, this time with the columns in the 'columns' parameter on the X-axis. \"\"\"\n",
    "def my_resid_plots(y_test, y_pred, X_test, columns): # residual plots for the features\n",
    "    residuals=y_test-y_pred\n",
    "    print(\"Heteroscendasticity hypothesis test p value:\", het_breuschpagan(resid=residuals, exog_het=X_test)[1])\n",
    "    for idx, column in enumerate(columns):\n",
    "        plt.scatter(X_test[:,idx], residuals, color=\"black\")\n",
    "        plt.title(\"Residual Plot for \"+ column)\n",
    "        plt.xlabel(column + \" Values\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.axhline(y=0, linewidth=3, color=\"black\")\n",
    "        plt.show() \n",
    "\n",
    "\"\"\" Plot of the Actual (observed) values on the X-axis and Predicted values on the Y-axis. \"\"\"\n",
    "def prediction_plot(y_test, y_pred, label_name=\"\"): # for showing actual values vs predicted values\n",
    "    plt.scatter(y_test, y_pred, color=\"black\")\n",
    "    plt.plot([x for x in range(0,int(np.max([np.max(y_test), np.max(y_pred)])))]) #straight line\n",
    "    plt.title(\"Actual vs Predicted \" + label_name + \" Plot\")\n",
    "    plt.xlabel(\"Actual \" + label_name + \" Values\")\n",
    "    plt.ylabel(\"Predicted \" + label_name + \" Values\")\n",
    "    plt.show()\n",
    "    \n",
    "\"\"\" Apply the boxcox transformation to the 'data' parameter and return the transformed dataset. 'data' can be a Pandas dataframe\n",
    "or a Numpy array. \"\"\"\n",
    "def my_normalizer(data, info_print=False):\n",
    "    if isinstance(data, pd.DataFrame): # if 'data' is a dataframe\n",
    "        new_df=data\n",
    "        transformed_columns=[]\n",
    "        for column in new_df.columns: # iterate over the columns in the dataframe\n",
    "            if np.min(new_df[column]>=1) and column!=\"eFG%\": # if the minimum value is greater than 1\n",
    "                new_df[column]=boxcox(player_data_df[column].values)[0] # apply the boxcox transformation to the column\n",
    "                transformed_columns.append(column) # and append it to the transformed_columns list\n",
    "        if info_print is True:\n",
    "            return new_df, transformed_columns\n",
    "        else:\n",
    "            return new_df\n",
    "    else: # if 'data' is a multi-dimentional array\n",
    "        new_array=data\n",
    "        transformed_indices=[]\n",
    "        for col_idx in range(new_array.shape[1]): # iterate over the columns (by cycling over the column index)\n",
    "            if np.min(new_array[:,col_idx]>=1):\n",
    "                new_array[:,col_idx]=boxcox(data[:,col_idx])[0]\n",
    "                transformed_indices.append(col_idx)\n",
    "        if info_print is True:\n",
    "            return new_array, transformed_indices\n",
    "        else:\n",
    "            return new_array  \n",
    "        \n",
    "\"\"\" For real world applications, checking against new data.\"\"\"\n",
    "def get_predictions(data, clf, columns=feat_selector.support_, pol_degree=2):\n",
    "    if isinstance(data, pd.DataFrame): #if 'data' is a dataframe \n",
    "        return clf.predict(PolynomialFeatures(degree=pol_degree).fit_transform(data)[:,columns])\n",
    "    else: #if 'data' is a list or numpy array\n",
    "        return clf.predict(PolynomialFeatures(degree=pol_degree).fit_transform(np.array(data))[:,columns])\n",
    "\n",
    "\"\"\" Getting a mean absolute error for the predictions in certain intervals (only useful if heteroscendasticity is present).\"\"\"\n",
    "\"\"\" CURRENTLY BROKEN, RETURNS ALL NaN \"\"\"\n",
    "def get_avg_error(y_test, y_pred, interval=1, simple_print=True):\n",
    "    min_, max_=(np.min(y_pred), np.max(y_pred))\n",
    "    n_steps=int((max_-min_)/interval)+1\n",
    "    avg_abs_error=[]\n",
    "    n_data_points=[]\n",
    "    residuals=np.abs(y_test-y_pred) #absolute errors\n",
    "    \n",
    "    for step in range(n_steps): # go over each interval, calculate the avg absolute error and # data points in interval\n",
    "        bool_indices=y_pred[(y_pred>=min_+step*interval) & (y_pred<=min_+(step+1)*interval)]\n",
    "        avg_abs_error.append(np.mean(residuals[bool_indices]))\n",
    "        n_data_points.append(np.sum(bool_indices))\n",
    "    \n",
    "    if simple_print is True:\n",
    "        print(avg_abs_error)\n",
    "    else:\n",
    "        for idx, error in enumerate(avg_abs_error):\n",
    "            print(\"[%f,%f] interval avg absolute error is %f, with %d data points\" \n",
    "                % ((min_+idx*interval),(min_+(idx+1)*interval), error, n_data_points[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Models for Drafted NBA Centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points per Game (PTS column) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eda=player_data_df[[\"Draft Placing\", \"eFG%\", \"Age\", \"MP\"]] # creating a dataframe of only the features selected from EDA.\n",
    "y=player_data_df[\"PTS\"] # the dependant variable\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_eda, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf_pts=LinearRegression().fit(X_train, y_train) #building/training the model\n",
    "y_pred=clf_pts.predict(X_test) # predicting on the test set\n",
    "\n",
    "performance_prints(clf_pts, X_train, X_test, y_train, y_test) #printing out the R^2 values for the training and testing sets\n",
    "my_resid_plot(y_test, y_pred, label_name=\"PTS\") # diagnosing the residuals of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical evaluation comments.** Though the R^2 is quite high, the residual plot shows us that our model is biased. That is, the values of the errors in our predicitions (residuals) are not independant of the predicted values. Furthermore there is distinct increase in the spread of the residuals as well as a U-shape. All of the prior mean that perhaps we need to transform our data to achieve a model that fits the data better. The next logical step would be include polynomial transformations of the existing features. Given the scatter plots of the independant variables against PTS, we saw at most 1 point of inflection, this means we only need to consider 2nd order polynomials. Another consideration is the curse of dimentionality that could arise if we considered 3rd order polynomial features. Let us try this approach and see if our model performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=PolynomialFeatures(degree=2).fit_transform(X_eda)\n",
    "y=player_data_df[\"PTS\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "clf_pts=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_pts.predict(X_test)\n",
    "\n",
    "performance_prints(clf_pts, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"PTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More technical commentary.** By introducing polynomial features we have succeeded in reducing bias (as demonstrated by a more random residual plot) and also reduced the MSE from 5.7 to 4.3, however **heteroscedasticity is still present** so we will need to deal with that at some point. Also, having computed 5-fold cross validation we can be confident in our model's ability to generalise to unseen data. We will now consider a larger subset of features from the dataframe (up to 2nd order polynomial features) and let the selector (RFECV) pick the best ones (the number of features is also chosen by RFECV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"print(\"Columns used before (with MSE=3.2):\", player_data_df.iloc[:,[2,3,4,5,6,9,10,11,12]].columns.values)\n",
    "print(\"Columns used in the final version:\", player_data_df.iloc[:,[3,5,6,10,12]].columns.values)\"\"\"\n",
    "player_data_pts=player_data_df.iloc[:,[3,5,6,10,12]] \n",
    "X=PolynomialFeatures(degree=2).fit_transform(player_data_pts)\n",
    "y=player_data_df[\"PTS\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "feat_selector=RFECV(LinearRegression(), cv=5).fit(X_train, y_train)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X[:,feat_selector.support_], y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf_pts=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_pts.predict(X_test)\n",
    "\n",
    "performance_prints(clf_pts, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"PTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is on average 1.886ppg away from the true value (square root of MSE of 3.557). However, given that the variance in the error (residual) is not constant across the predicted value range, we have to be careful about our confidence in the predicted values. For example if we got a 4ppg (points per game) prediction for a certain player, we can be pretty certain that the maximum deviation from that value would be +-2.5ppg and an average that is much smaller than 1.886. Likewise if we got a prediction of 20ppg, this time the maximum deviation from the value would be +-7.5ppg and the average would be much greater than 1.886. \n",
    "\n",
    "The general point that we should be aware of, is that given the presence of heteroscendasticity (non-constant residual variance) and the fact that there are alot more 'average' players (0-10ppg) than 'above average players' (>10ppg), we cannot use average/means are reliable summary statistics (mean is sensitive to outliers and doesn't account for heteroscendasticity).\n",
    "\n",
    "Of course, the statistic/graph of interest is dependant on the use case, but probably the most useful way to evaluate the prediction of a model is to provide the residual plot along with a box-plot, and the number of observations for each interval in predicted PTS values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another technical point**. Heteroscendasticity is still present, so while this does not make our model unbiased it makes the inferences abit more difficult (as mentioned above). I've rescaled/normalized the data, tried weighted OLS and neither of those approaches introduced homoscendasticity. Finally I applied a boxcox transformation on the dependant variable (PTS) and while this managed to produce homoscendasticity, it ruined the interpretability/usefulness of the model as reversing the transformation of the predicted values yealds the heteroscendasticity once again. Hence, for the purposes of making the model useful, I've not transformed the dependant variable, but have been very careful to constantly signpost the presence of non-constant residual variance. Given that the models to come show similar problems, I will not go into as much detail for each model as I did here, but make it very clear that I am aware of the problems that some residual plots illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assists per Game (AST column) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"print(\"Columns used before:\", player_data_df.iloc[:,[0,1,3,4,5,6,7,8,9,10,11,12]].columns.values)\n",
    "print(\"Columns used in the final version:\", player_data_df.iloc[:,[3,5,6,10]].columns.values)\"\"\"\n",
    "player_data_ast=player_data_df.iloc[:,[3,5,6,10]]\n",
    "X=PolynomialFeatures(degree=2).fit_transform(player_data_ast)\n",
    "y=player_data_df.AST\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "feat_selector=RFECV(LinearRegression(), cv=5).fit(X_train, y_train)\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X[:,feat_selector.support_], y, test_size=0.3, random_state=42)\n",
    "clf_ast=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_ast.predict(X_test)\n",
    "\n",
    "performance_prints(clf_ast, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"AST\")\n",
    "#prediction_plot(y_test, y_pred, label_name=\"AST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'lines' that you we see a result of AST values being registered in 0.1APG increments (0.1, 0.2, 0.3 etc) and this, considering the moderately small scale of values, introduces a certain level of 'discreteness' in the values both visually and numerically. The same will also be true of the STL residual plot, and the BLK residual plot (for the same reasons)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Rebounds per Game (TRB column) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"print(\"Columns used before:\", player_data_df.iloc[:,[0,1,2,3,4,5,6,8,9,10,12]].columns.values)\n",
    "print(\"Columns used in the final version:\", player_data_df.iloc[:,[3,5,6,10]].columns.values)\"\"\"\n",
    "X=PolynomialFeatures(degree=2).fit_transform(player_data_df.iloc[:,[3,4,5,6,8,9,10,12]])\n",
    "y=player_data_df.TRB\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf_trb=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_trb.predict(X_test)\n",
    "\n",
    "performance_prints(clf_trb, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"TRB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks per Game (BLK column) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"print(\"Columns used before:\", player_data_df.iloc[:,[0,2,3,5,6,8,10,11]].columns.values)\n",
    "print(\"Columns used in the final version:\", player_data_df.iloc[:,[3,5,6,10]].columns.values)\"\"\"\n",
    "X=PolynomialFeatures(degree=2).fit_transform(player_data_df.iloc[:,[3,5,6,8,10]])\n",
    "y=player_data_df.BLK\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf_blk=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_blk.predict(X_test)\n",
    "\n",
    "performance_prints(clf_blk, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"BLK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Steals per Game (STL column) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print(\"Columns used before:\", player_data_df.iloc[:,[0,1,2,3,4,5,6,8,10,11,12]].columns.values)\n",
    "print(\"Columns used in the final version:\", player_data_df.iloc[:,[3,5,6,10]].columns.values)\"\"\"\n",
    "X=PolynomialFeatures(degree=2).fit_transform(player_data_df.iloc[:,[3,5,6,10]])\n",
    "y=player_data_df.STL\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf_stl=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_stl.predict(X_test)\n",
    "\n",
    "performance_prints(clf_stl, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"STL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undrafted Player Models\n",
    "### Points per Game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=player_data_df_undrafted.iloc[:,[3,5,9,11]].values\n",
    "y=player_data_df_undrafted[\"PTS\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "clf_pts=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_pts.predict(X_test)\n",
    "\n",
    "performance_prints(clf_pts, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"PTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assists per Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_data_ast=player_data_df_undrafted.iloc[:,[3,5,9]]\n",
    "X=PolynomialFeatures(degree=2).fit_transform(player_data_ast)\n",
    "y=player_data_df_undrafted.AST\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "feat_selector=RFECV(LinearRegression(), cv=5).fit(X_train, y_train)\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X[:,feat_selector.support_], y, test_size=0.3, random_state=42)\n",
    "clf_ast=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_ast.predict(X_test)\n",
    "\n",
    "performance_prints(clf_ast, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"AST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Rebounds per Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=player_data_df_undrafted.iloc[:,[2,3,4,5,7,8,9,11]]\n",
    "y=player_data_df_undrafted.TRB\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf_trb=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_trb.predict(X_test)\n",
    "\n",
    "performance_prints(clf_trb, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"TRB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks per Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=PolynomialFeatures(degree=2).fit_transform(player_data_df_undrafted.iloc[:,[3,5,7,9]])\n",
    "y=player_data_df_undrafted.BLK\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf_blk=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_blk.predict(X_test)\n",
    "\n",
    "performance_prints(clf_blk, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"BLK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steals per Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=PolynomialFeatures(degree=2).fit_transform(player_data_df_undrafted.iloc[:,[3,5,10]])\n",
    "y=player_data_df_undrafted.STL\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf_stl=LinearRegression().fit(X_train, y_train)\n",
    "y_pred=clf_stl.predict(X_test)\n",
    "\n",
    "performance_prints(clf_stl, X_train, X_test, y_train, y_test)\n",
    "my_resid_plot(y_test, y_pred, label_name=\"STL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical\n",
    "The regression models for Points and Rebounds are fairly good in terms of predictive accuracy, however models for  Assists, Blocks and Steals (as mentioned earlier) are much less accurate. There are a few reasons for this. The numerical aspect: values are very small and more 'discrete' than continuous (0.1, 0.2, 0.3 etc). Inherent randomness/rarity: these statistics are inherently more difficult to predict due to the 'randomness' in Steals, the rarity in Blocks, and the fact that centers aren't usually the ones assisting (while this isn't exactly true in all cases with the game having evolved alot in the last 20 years, it's still an uncommon ask of a center to be assisting others, leading to some inherent randomness in the values). Fundamentally though, it's either a lack of features (which were not scraped) that might have provided greater models for Assists, Steals and Blocks, or perhaps it's inherently much harder to predict due to the beforementioned reasons.\n",
    "\n",
    "With regards to models for Points and Rebounds (which performed the best), feature normalization and weighted least squares did not aid in getting rid of heteroscendasticity, however this just means we need to be more careful in our statistical inferences.\n",
    "\n",
    "### Non-Technical\n",
    "\n",
    "The models for Points per Game and Rebounds per Game are sufficiently accurate to perhaps use in a real world setting for prediction. However, as mentioned before, given that the use cases of the models might differ (along with specific accuracy requirements) it is left for the user to evaluate the residual plots of the model and make their own conclusions as to if it meets their requirements. Custom functions have been made to make it as easy as possible for the user to get predictions for unseen data and to get confidence intervals for the predictions. Predictions is obviously also possible for Assists, Steals and Blocks, however these models are far less accurate.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
