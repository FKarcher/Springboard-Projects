{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Brief exercise formulation:\n",
    "Using data in file 'data/world_bank_projects.json'\n",
    "1.Find the 10 countries with most projects \n",
    "2.Find the top 10 major project themes (using column 'mjtheme_namecode') \n",
    "In 2. above you will notice that some entries have only the code and the name is missing. Create a dataframe with the missing names filled in.\n",
    "Solution code is found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing necessary modules \n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading in json file as a Dataframe from the exercise folder located on my desktop\n",
    "projects_df=pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/data_wrangling_json/data/world_bank_projects.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the necessary modules I proceeded to read in the .json file as a local file. I didn't read it in as a URL as the URL to the Data source of the json took me to a Wordpress login page. Given the assumption that access to the file via a URL was not a possible I read it in locally. I recognise that for the code to run on another machine the file path will need to be re-specified as appropriate and hence reading the json from a URL would have solved that issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 50 columns):\n",
      "_id                         500 non-null object\n",
      "approvalfy                  500 non-null int64\n",
      "board_approval_month        500 non-null object\n",
      "boardapprovaldate           500 non-null object\n",
      "borrower                    485 non-null object\n",
      "closingdate                 370 non-null object\n",
      "country_namecode            500 non-null object\n",
      "countrycode                 500 non-null object\n",
      "countryname                 500 non-null object\n",
      "countryshortname            500 non-null object\n",
      "docty                       446 non-null object\n",
      "envassesmentcategorycode    430 non-null object\n",
      "grantamt                    500 non-null int64\n",
      "ibrdcommamt                 500 non-null int64\n",
      "id                          500 non-null object\n",
      "idacommamt                  500 non-null int64\n",
      "impagency                   472 non-null object\n",
      "lendinginstr                495 non-null object\n",
      "lendinginstrtype            495 non-null object\n",
      "lendprojectcost             500 non-null int64\n",
      "majorsector_percent         500 non-null object\n",
      "mjsector_namecode           500 non-null object\n",
      "mjtheme                     491 non-null object\n",
      "mjtheme_namecode            500 non-null object\n",
      "mjthemecode                 500 non-null object\n",
      "prodline                    500 non-null object\n",
      "prodlinetext                500 non-null object\n",
      "productlinetype             500 non-null object\n",
      "project_abstract            362 non-null object\n",
      "project_name                500 non-null object\n",
      "projectdocs                 446 non-null object\n",
      "projectfinancialtype        500 non-null object\n",
      "projectstatusdisplay        500 non-null object\n",
      "regionname                  500 non-null object\n",
      "sector                      500 non-null object\n",
      "sector1                     500 non-null object\n",
      "sector2                     380 non-null object\n",
      "sector3                     265 non-null object\n",
      "sector4                     174 non-null object\n",
      "sector_namecode             500 non-null object\n",
      "sectorcode                  500 non-null object\n",
      "source                      500 non-null object\n",
      "status                      500 non-null object\n",
      "supplementprojectflg        498 non-null object\n",
      "theme1                      500 non-null object\n",
      "theme_namecode              491 non-null object\n",
      "themecode                   491 non-null object\n",
      "totalamt                    500 non-null int64\n",
      "totalcommamt                500 non-null int64\n",
      "url                         500 non-null object\n",
      "dtypes: int64(7), object(43)\n",
      "memory usage: 195.4+ KB\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Getting an overview of the structure of the DataFrame\n",
    "print(projects_df.info(),\"\\n\")\n",
    "\n",
    "#Filtering the DataFrame to keep only the relevant columns for the task of finding the top 10 countries by number of projects\n",
    "projects_df=projects_df[[\"mjtheme\",\"project_name\",\"mjtheme_namecode\",\"theme_namecode\",\"countryname\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of a Google Chrome extension 'JSON Formatter' and the .info() call I saw that; alot of the columns could be discarded for the purposes of the exercise, there were numerous empty string entries (\"\") and that there were 500 rows of input. Hence, the filtering of the DataFrame. Granted, in retrospect, it could also have been possible to keep only a single column with no empty string entries (the 'countryname' column) and the 'mjtheme_namecode' column which would be required for the further parts of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Countries by Project Count \n",
      " Republic of Indonesia              19\n",
      "People's Republic of China         19\n",
      "Socialist Republic of Vietnam      17\n",
      "Republic of India                  16\n",
      "Republic of Yemen                  13\n",
      "People's Republic of Bangladesh    12\n",
      "Nepal                              12\n",
      "Kingdom of Morocco                 12\n",
      "Republic of Mozambique             11\n",
      "Africa                             11\n",
      "Name: countryname, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count how many rows (projects) each other Country appears in, then sort descending and slice top 10\n",
    "\n",
    "print(\"Top 10 Countries by Project Count\",\"\\n\", projects_df[\"countryname\"].value_counts().sort_values(ascending=False)[:10],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call above sliced a Pandas series of the top 10 countries by project count. I used .value_counts() as opposed to .groupby(\"countryname\")[some_other_column_name] due to its elegance and no need to aggregate by another column. I chained .sort_values() at the end to avoid an unnecessary line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a list of lists of values from the mjtheme_namecode column\n",
    "list_of_lists=list(projects_df[\"mjtheme_namecode\"].values)\n",
    "\n",
    "#and generate a list of dictionaries of code:name pairs to use as input for json_normalize\n",
    "list_of_dicts=[dict(key_value_pair) for json_list in list_of_lists for key_value_pair in json_list]\n",
    "\n",
    "#Creating a Dataframe of theme names (name column) and their code (code column)\n",
    "major_themes=json_normalize(list_of_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The json_normalize requires a dictionary of a list of dictionaries as input to first parameter. Hence the previous 2 lines of code. The first extracts a numpy array from a Pandas series via the .values attribute which is then converted to a list (list_of_lists). The second is a dictionary comprehension that loops over the list of lists (entries in the mjtheme_namecode), and the inner for loop loops over the sets of code:name (key:value) pairs and casts them to a dictionary before appending to list_of_dicts. Then, finally the call to json_normalize() is made to create a DataFrame of only the 'code' and 'name' columns. This approach to producing a DataFrame for the 'mjtheme_namecode' column seemed quite inelegant, but I couldn't find another way to do it given the expected parameter type of json_normalize and the requirement by the exercise formulation to use the method (\"...and the techniques demonstrated above\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 unique 'major themes codes'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing how many unique codes there actually are.\n",
    "print(\"There are %d unique 'major themes codes'\\n\" % (len(major_themes[\"code\"].unique())))\n",
    "\n",
    "#Given the small number of unique codes we can consider them as a category.\n",
    "major_themes[\"code\"]=major_themes[\"code\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the small range of values of the code column (the first method call above) it made sense to consider it as a categorical variable for the purposes of saving space and speeding up computations, hence the conversion to categorical via .astype(\"category\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'major_themes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4e5da0975a02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Define a dictionary to be used in the function replace_emptry_entries.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munique_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmajor_themes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmajor_themes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmajor_themes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#The function that will aid us in filling in rows with code column entries, but no name column entries.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreplace_empty_entries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'major_themes' is not defined"
     ]
    }
   ],
   "source": [
    "#Define a dictionary to be used in the function replace_emptry_entries.\n",
    "unique_dict={code:name for code in major_themes[\"code\"].unique() for name in major_themes.loc[major_themes[\"code\"]==code,\"name\"].values if name!=\"\"}\n",
    "\n",
    "#The function that will aid us in filling in rows with code column entries, but no name column entries.\n",
    "def replace_empty_entries(row):\n",
    "    return unique_dict[row[\"code\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce a Dataframe of the 'mjtheme_namecode' column with no emptry string entried in the 'name' column I created a dictionary unique_dict which contained unique code:name pairs so that in my custom made function for any value in the 'code' column I could access a unique 'name' column value from unique_dict and hence redefine the 'name' column. By doing this I fulfill the 3rd requirement of the exercise. The use of .apply() as opposed to using a for loop is once again based on a speed consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'major_themes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e5089283b277>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Applying the function above to generate a column with no empty string values and convert to categorical variable to save space.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmajor_themes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmajor_themes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace_empty_entries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"category\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#As required by point 3 in the exercises, here is the print output of the complete code:name DataFrame.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor_themes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'major_themes' is not defined"
     ]
    }
   ],
   "source": [
    "#Applying the function above to generate a column with no empty string values and convert to categorical variable to save space.\n",
    "major_themes[\"name\"]=major_themes.apply(replace_empty_entries,axis=1).astype(\"category\")\n",
    "\n",
    "#As required by point 3 in the exercises, here is the print output of the complete code:name DataFrame.\n",
    "print(major_themes,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'major_themes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-96cb84bdf063>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Print output of the top 10 major project themes by count in descending order.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Top 10 Major Themes\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmajor_themes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'major_themes' is not defined"
     ]
    }
   ],
   "source": [
    "#Print output of the top 10 major project themes by count in descending order.\n",
    "print(\"Top 10 Major Themes\",\"\\n\", major_themes[\"name\"].value_counts().sort_values(ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same reasoning as before, the 'name' column values are converted to categorical. The call after prints the complete DataFrame to highlight the successful transformation. Then (like before) we find the top 10 countries by count of project, top 10 project themes and print them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code omits certain DataFrame prints and a few intermediate steps while trying to understand the structure of the json file and so on as it does not contribute to the solution of the exercise. Hopefully this provides enough justification for the programming choices made. I can't help but feel that there are possibly more elegant solutions that don't require creation of separate dictionaries for example to achieve the same result so do feel free to share those suggestions. Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
