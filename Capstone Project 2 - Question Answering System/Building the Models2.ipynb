{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/training_data.json\").reset_index(drop=True)\n",
    "dev_df=pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/validation_data.json\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization\n",
    "To be able to use words, phrases, questions or other natural language constructs in our model we require a to provide our neural network a numerical representation of our words (as these are the elemental NLP 'particles'). The simplest implementation would be to use 'one hot encoding' and define each word as a vector the size of our dictionary (the number of unique words found in our collection of documents, our corpus). However, this approach will most likely be insufficient for the purposes of a question answering system. word2vec and GloVe are 2 popular choices sophisticated options for word embeddings that also capture word similarities. I will not go into the details of either architecture other than to say that we will not be re-training the word vectors due to the insufficient size of the dataset, and we will begin with the GloVe word embeddings due to it's superior performance in most 'downstream' modelling tasks. Having said that, given the simplicity of swapping word vector representations we will also test out performance with word2vec (providing we can do so in a time-efficient manner).\n",
    "\n",
    "Info and download links for GloVe can be found at: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Custom Functions\n",
    "For the purpose of not repeating code, avoiding bugs and developing good programming practice/design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Takes a URL or a local path and returns a dictionary of GloVe word vectors where the key is the word and the value is the \n",
    "word vector with the dimension specified in the input file.\"\"\"\n",
    "def get_word_vector_dict(url_or_path):\n",
    "    \n",
    "    with open(url_or_path, encoding=\"utf8\") as glove_text:\n",
    "        word_lines=glove_text.readlines()\n",
    "    word_embeddings=[line.split(\" \") for line in word_lines]\n",
    "    word_vector_dict={element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "    \n",
    "    return word_vector_dict\n",
    "\n",
    "\n",
    "\"\"\"Takes a URL or path like the previous function, or can take a word vector dictionary and returns a word vector\n",
    "dataframe. Rows of the dataframe are the word vectors, columns are the dimensions of the word vector, indices are the words.\"\"\"\n",
    "def get_word_vector_df(url_path_or_dict):\n",
    "    \n",
    "    if type(url_path_or_dict) is str:\n",
    "        with open(url_path_or_dict, encoding=\"utf8\") as glove_text:\n",
    "            word_lines=glove_text.readlines()\n",
    "        word_embeddings=[line.split(\" \") for line in word_lines]\n",
    "        word_vector_dict={element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "        word_vector_df=pd.DataFrame(word_vector_dict).transpose()\n",
    "    \n",
    "    else:\n",
    "        word_vector_df=pd.DataFrame(url_path_or_dict).transpose()\n",
    "    \n",
    "    return word_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Word Vectors\n",
    "As mentioned above with regards to what model we use for the word vectors, it's important to note that the dimention of the word vectors is a hyperparameter of the Neural Networks to come, so to keep our options open we imported a few different word vectors representations and the custom functions defined above make this a 'one line of code' affair (dictionary or dataframe).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_50_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.50d.txt\")\n",
    "word_vector_50_df=get_word_vector_df(word_vector_50_dict)\n",
    "vocab=np.array(word_vector_50_dict.keys()) #400k words as per the documentation.\n",
    "word_vector_100_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.100d.txt\")\n",
    "word_vector_100_df=get_word_vector_df(word_vector_100_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 400000 entries, ! to ï¿¥\n",
      "Data columns (total 50 columns):\n",
      "0     400000 non-null float64\n",
      "1     400000 non-null float64\n",
      "2     400000 non-null float64\n",
      "3     400000 non-null float64\n",
      "4     400000 non-null float64\n",
      "5     400000 non-null float64\n",
      "6     400000 non-null float64\n",
      "7     400000 non-null float64\n",
      "8     400000 non-null float64\n",
      "9     400000 non-null float64\n",
      "10    400000 non-null float64\n",
      "11    400000 non-null float64\n",
      "12    400000 non-null float64\n",
      "13    400000 non-null float64\n",
      "14    400000 non-null float64\n",
      "15    400000 non-null float64\n",
      "16    400000 non-null float64\n",
      "17    400000 non-null float64\n",
      "18    400000 non-null float64\n",
      "19    400000 non-null float64\n",
      "20    400000 non-null float64\n",
      "21    400000 non-null float64\n",
      "22    400000 non-null float64\n",
      "23    400000 non-null float64\n",
      "24    400000 non-null float64\n",
      "25    400000 non-null float64\n",
      "26    400000 non-null float64\n",
      "27    400000 non-null float64\n",
      "28    400000 non-null float64\n",
      "29    400000 non-null float64\n",
      "30    400000 non-null float64\n",
      "31    400000 non-null float64\n",
      "32    400000 non-null float64\n",
      "33    400000 non-null float64\n",
      "34    400000 non-null float64\n",
      "35    400000 non-null float64\n",
      "36    400000 non-null float64\n",
      "37    400000 non-null float64\n",
      "38    400000 non-null float64\n",
      "39    400000 non-null float64\n",
      "40    400000 non-null float64\n",
      "41    400000 non-null float64\n",
      "42    400000 non-null float64\n",
      "43    400000 non-null float64\n",
      "44    400000 non-null float64\n",
      "45    400000 non-null float64\n",
      "46    400000 non-null float64\n",
      "47    400000 non-null float64\n",
      "48    400000 non-null float64\n",
      "49    400000 non-null float64\n",
      "dtypes: float64(50)\n",
      "memory usage: 155.6+ MB\n",
      "None\n",
      "                  0              1              2              3   \\\n",
      "count  400000.000000  400000.000000  400000.000000  400000.000000   \n",
      "mean       -0.129201      -0.288662      -0.012249      -0.056767   \n",
      "std         0.636704       0.613996       0.612466       0.624203   \n",
      "min        -5.459300      -3.630000      -3.883500      -4.625200   \n",
      "25%        -0.534942      -0.683263      -0.407460      -0.463890   \n",
      "50%        -0.133555      -0.315320      -0.009604      -0.082614   \n",
      "75%         0.274582       0.079589       0.386820       0.325983   \n",
      "max         3.874000       4.018300       3.977500       5.310100   \n",
      "\n",
      "                  4              5              6              7   \\\n",
      "count  400000.000000  400000.000000  400000.000000  400000.000000   \n",
      "mean       -0.202111      -0.083890       0.333597       0.160451   \n",
      "std         0.639919       0.665588       0.623440       0.608380   \n",
      "min        -4.054900      -5.309800      -3.427100      -3.934900   \n",
      "25%        -0.611740      -0.492560      -0.059535      -0.213180   \n",
      "50%        -0.207160      -0.074543       0.348130       0.189620   \n",
      "75%         0.205995       0.333940       0.737252       0.564470   \n",
      "max         4.601400       4.868700       3.863700       3.627900   \n",
      "\n",
      "                  8              9       ...                   40  \\\n",
      "count  400000.000000  400000.000000      ...        400000.000000   \n",
      "mean        0.038675       0.178331      ...             0.139920   \n",
      "std         0.645970       0.619217      ...             0.570008   \n",
      "min        -3.792700      -3.816300      ...            -3.910200   \n",
      "25%        -0.360852      -0.204772      ...            -0.220583   \n",
      "50%         0.043624       0.195080      ...             0.133390   \n",
      "75%         0.443952       0.576370      ...             0.490880   \n",
      "max         3.973400       3.787900      ...             3.923000   \n",
      "\n",
      "                  41             42             43             44  \\\n",
      "count  400000.000000  400000.000000  400000.000000  400000.000000   \n",
      "mean       -0.174463      -0.080118       0.084952      -0.010416   \n",
      "std         0.608277       0.608691       0.629454       0.627500   \n",
      "min        -3.484900      -3.139300      -3.788800      -5.194100   \n",
      "25%        -0.576230      -0.474930      -0.320230      -0.408950   \n",
      "50%        -0.201420      -0.092212       0.097806      -0.012346   \n",
      "75%         0.199200       0.308303       0.500650       0.388230   \n",
      "max         4.508700       3.948300       5.075600       3.532800   \n",
      "\n",
      "                  45             46             47             48  \\\n",
      "count  400000.000000  400000.000000  400000.000000  400000.000000   \n",
      "mean       -0.137049       0.201271       0.100693       0.006530   \n",
      "std         0.620790       0.558487       0.615509       0.600067   \n",
      "min        -3.513300      -3.077500      -4.723100      -4.615100   \n",
      "25%        -0.532460      -0.151080      -0.285080      -0.373643   \n",
      "50%        -0.156920       0.205290       0.102910       0.001320   \n",
      "75%         0.238690       0.557160       0.485930       0.385100   \n",
      "max         4.589400       4.234500       4.541900       4.822800   \n",
      "\n",
      "                  49  \n",
      "count  400000.000000  \n",
      "mean        0.016851  \n",
      "std         0.623284  \n",
      "min        -3.915300  \n",
      "25%        -0.392800  \n",
      "50%         0.002287  \n",
      "75%         0.416022  \n",
      "max         4.178500  \n",
      "\n",
      "[8 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Some quick test prints/sanity checks to make sure that everything is ok and we haven't made any errors. It is also\n",
    "important to note that this is mostly for the reader, much of the testing of code/test prints are done, for this stage at \n",
    "least.\"\"\"\n",
    "print(word_vector_50_df.info())\n",
    "print(word_vector_50_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Reading in the pre-trained 50-dimentional GloVe vectors\\nwith open(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.50d.txt\", encoding=\"utf8\") as glove_50d_text:\\n    word_lines=glove_50d_text.readlines()\\n    \\n# Array of arrays of words and their word vector values.\\nword_embeddings=[line.split(\" \") for line in word_lines]\\n# Word vector dictionary. Words are the keys, word vectors are the values.\\nword_vector_dict={element[0]:list(map(float, element[1:])) for element in word_embeddings}\\n# Word vector dataframe. Word vectors are rows, words are the indices, columns are the dimentions of the word vectors. \\nword_vector_df=pd.DataFrame(word_vector_dict).transpose()'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Original code for reading in an composing the dataframe for the word vectors.\"\"\"\n",
    "\n",
    "\"\"\"# Reading in the pre-trained 50-dimentional GloVe vectors\n",
    "with open(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.50d.txt\", encoding=\"utf8\") as glove_50d_text:\n",
    "    word_lines=glove_50d_text.readlines()\n",
    "    \n",
    "# Array of arrays of words and their word vector values.\n",
    "word_embeddings=[line.split(\" \") for line in word_lines]\n",
    "# Word vector dictionary. Words are the keys, word vectors are the values.\n",
    "word_vector_dict={element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "# Word vector dataframe. Word vectors are rows, words are the indices, columns are the dimentions of the word vectors. \n",
    "word_vector_df=pd.DataFrame(word_vector_dict).transpose()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
