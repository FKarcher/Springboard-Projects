{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, Masking, Lambda, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from attention_gru import SoftAttnGRU as AttentionGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter/TO DO Notes\n",
    "- Word vector size (50,100,200,300)\n",
    "- Still need to find a way feed in all of the sentences into input module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/training_data.json\").reset_index(drop=True)\n",
    "dev_df = pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/validation_data.json\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization\n",
    "To be able to use words, phrases, questions or other natural language constructs in our model we require a to provide our neural network a numerical representation of our words (as these are the elemental NLP 'particles'). The simplest implementation would be to use 'one hot encoding' and define each word as a vector the size of our dictionary (the number of unique words found in our collection of documents, our corpus). However, this approach will most likely be insufficient for the purposes of a question answering system. word2vec and GloVe are 2 popular choices sophisticated options for word embeddings that also capture word similarities. I will not go into the details of either architecture other than to say that we will not be re-training the word vectors due to the insufficient size of the dataset, and we will begin with the GloVe word embeddings due to it's superior performance in most 'downstream' modelling tasks. Having said that, given the simplicity of swapping word vector representations we will also test out performance with word2vec (providing we can do so in a time-efficient manner).\n",
    "\n",
    "Info and download links for GloVe can be found at: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Word Vector' Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_vector_dict(url_or_path):\n",
    "    \"\"\"Takes a URL or a local path and returns a dictionary of GloVe word vectors where the key is the word and the value is the \n",
    "    word vector with the dimension specified in the input file.\"\"\"\n",
    "    with open(url_or_path, encoding=\"utf8\") as glove_text:\n",
    "        word_embeddings = [line.split(\" \") for line in glove_text.readlines()]\n",
    "    word_vector_dict = {element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "    \n",
    "    return word_vector_dict\n",
    "\n",
    "\n",
    "def get_word_vector_df(url_path_or_dict):\n",
    "    \"\"\"Takes a URL or path like the previous function, or can take a word vector dictionary and returns a word vector dataframe.\n",
    "    Rows of the dataframe are the word vectors, columns are the dimensions of the word vector, indices are the words.\"\"\"\n",
    "    if type(url_path_or_dict) is str:\n",
    "        with open(url_path_or_dict, encoding=\"utf8\") as glove_text:\n",
    "            word_embeddings = [line.split(\" \") for line in glove_text.readlines()]\n",
    "        word_vector_dict = {element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "        word_vector_df = pd.DataFrame(word_vector_dict).transpose()\n",
    "    \n",
    "    else:\n",
    "        word_vector_df = pd.DataFrame(url_path_or_dict).transpose()\n",
    "    \n",
    "    return word_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Word Vectors\n",
    "As mentioned above with regards to what model we use for the word vectors, it's important to note that the dimention of the word vectors is a hyperparameter of the Neural Networks to come, so to keep our options open we imported a few different word vectors representations and the custom functions defined above make this a 'one line of code' affair (dictionary or dataframe).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector_50_dict = get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.50d.txt\")\n",
    "vocab = np.array(word_vector_50_dict.keys()) #400k words as per the documentation.\n",
    "word_vector_100_dict = get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.100d.txt\")\n",
    "\n",
    "#word_vector_200_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.200d.txt\")\n",
    "#word_vector_300_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Embedding, Padding Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    \"\"\"Takes a string (either context, question or answer) and returns a list of tokens.\"\"\"\n",
    "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in word_tokenize(string)]\n",
    "    \n",
    "    split_tokens = []\n",
    "    for token in tokens:\n",
    "        split_tokens.extend(re.split('(\\W+)', token))\n",
    "    return [token for token in split_tokens if token!=\" \" and token!=\"\"]\n",
    "\n",
    "\n",
    "def string_to_embedding(string, word_vector_dict=word_vector_50_dict):\n",
    "    \"\"\"Takes a context, question or answer and returns a list of word tokens in their vectorized form (embedding).\n",
    "    Updated to accept tokenizations for faster training.\"\"\"\n",
    "    tokens=[]\n",
    "    embedding=[]\n",
    "    \n",
    "    if isinstance(string, str) is True:\n",
    "        tokens = np.array(tokenize(string))\n",
    "    else:\n",
    "        tokens = np.array(string)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in word_vector_dict.keys():\n",
    "            embedding.extend(word_vector_dict[token])\n",
    "        else:\n",
    "            # We are vectorizing words with no embedding with the 'unk' token vectorization (already in GloVe)\n",
    "            embedding.extend(word_vector_dict[\"unk\"])\n",
    "    \n",
    "    return np.array(embedding)\n",
    "\n",
    "\n",
    "def get_sent_end_idx(context_tokenizations):\n",
    "    \"\"\" Get indices of tokens that are '.' (sentence end tokens). For one or many contexts.\"\"\"\n",
    "    return np.array( [np.where(np.array(context)==\".\") for context in context_tokenizations] )\n",
    "\n",
    "\n",
    "def get_padded_contexts(dataframe_indices_or_contexts):\n",
    "    \"\"\"Take a list of indices or a list of contexts and return a list of padded context embeddings ready for the Input Module.\"\"\"\n",
    "    context_index_pairs = list(zip(train_df.context.values, train_df.index.values))\n",
    "    padded_contexts_fresh = []\n",
    "    \n",
    "    if isinstance(dataframe_indices_or_contexts, int): # if a single index is provided\n",
    "        context = context_index_pairs[dataframe_indices_or_contexts][0]\n",
    "        padding_index = int(np.where(unique_contexts==context)[0])\n",
    "        padded_contexts_fresh.append(unique_padded_contexts[padding_index])\n",
    "        \n",
    "    elif isinstance(dataframe_indices_or_contexts[0], int): # if we are given a list of indices.\n",
    "          for index in dataframe_indices_or_contexts:\n",
    "                context = context_index_pairs[index][0]\n",
    "                padding_index = int(np.where(unique_contexts==context)[0])\n",
    "                padded_contexts_fresh.append(unique_padded_contexts[padding_index])\n",
    "    \n",
    "    elif isinstance(dataframe_indices_or_contexts[0], str): # if a list of contexts or a single context is provided\n",
    "        if len(dataframe_indices_or_contexts[0])!=1: # if a list of contexts\n",
    "            for context in dataframe_indices_or_contexts:\n",
    "                padding_index = int(np.where(unique_contexts==context)[0])\n",
    "                padded_contexts_fresh.append(unique_padded_contexts[padding_index])   \n",
    "        else: # if a single context\n",
    "            padding_index = int(np.where(unique_contexts==dataframe_indices_or_contexts)[0])\n",
    "            padded_contexts_fresh.append(unique_padded_contexts[padding_index])\n",
    "    \n",
    "    else:\n",
    "        print(\"NO EMBEDDING PROVIDED, INCORRECT PARAMETER DATA TYPE\")\n",
    "      \n",
    "    return np.array(padded_contexts_fresh).reshape(len(padded_contexts_fresh), -1, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk tokenizer generated around 110,00 unique tokens from our contexts, questions and answers in our dataset. ~31,000 of those tokens did not have pre-trained word vectorizations in the GloVe model. Some of these tokens were numbers, expressed as strings in an unfamiliar format, some of these tokens were misspelled words, some of these tokens were works in other languages, or symbols from other alphibets and so on. \n",
    "With the 'regex inspired' split in the tokenized function, we were able to reduce the number of words with no embeddings to around 16,000. To deal with the remaining words with no embeddings we assigned to them the embedding for the token *'unk'*, which by definition is the embedding for unknown words provided by GloVe. Thus any word/token that did not have an embedding got an *'unk'* embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing, Embedding and Padding Contexts, Questions and Answers\n",
    "In this section we seperate out the contexts, questions and answers, we embed finally embed all our words into \n",
    "vector representations and pad the sequences to fulfil Keras' input requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#padded_contexts_full=get_padded_contexts(train_df.index) Still can't fix the memory issue\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts, questions, answers = (train_df.context.values, train_df.question.values, train_df.answer_text.values)\n",
    "\n",
    "# Context Stuff (we embed and pad unique contexts and not all, due to memory limits)\n",
    "contexts_tokenized = np.array([tokenize(context) for context in contexts])\n",
    "#embedded_contexts=[string_to_embedding(tokenized_context) for tokenized_context in contexts_tokenized]\n",
    "unique_contexts = train_df.context.unique()\n",
    "unique_contexts_tokenized = np.array([tokenize(context) for context in unique_contexts])\n",
    "max_context_len = np.max([len(context) for context in unique_contexts_tokenized])\n",
    "unique_embedded_contexts = [string_to_embedding(tokenized_context) for tokenized_context in unique_contexts_tokenized]  \n",
    "unique_padded_contexts = pad_sequences(unique_embedded_contexts, max_context_len*50, padding=\"post\",\n",
    "                              dtype=np.dtype('f4')).reshape(len(unique_contexts),-1,50) \n",
    "\n",
    "# Questions Stuff\n",
    "questions_tokenized = np.array([tokenize(question) for question in questions])\n",
    "max_question_len = np.max([len(question) for question in questions_tokenized])\n",
    "embedded_questions = np.array([string_to_embedding(tokenized_question) for tokenized_question in questions_tokenized])\n",
    "padded_questions = pad_sequences(embedded_questions, max_question_len*50, padding=\"post\",\n",
    "                               dtype=\"float32\").reshape(len(questions),-1,50)\n",
    "\n",
    "# Answers Stuff\n",
    "answers_tokenized = np.array([tokenize(answer) for answer in answers])\n",
    "embedded_answers = np.array([string_to_embedding(tokenized_answer) for tokenized_answer in answers_tokenized])\n",
    "max_answer_len = np.max([len(answer) for answer in answers_tokenized])\n",
    "\n",
    "# Other useful variables\n",
    "sent_end_indeces = get_sent_end_idx(contexts_tokenized)\n",
    "sentence_lengths = sentence_lengths=[len(indices[0]) for indices in sent_end_indeces]\n",
    "max_num_sentences = np.max(sentence_lengths)\n",
    "\"\"\"#padded_contexts_full=get_padded_contexts(train_df.index) Still can't fix the memory issue\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dynamic Memory Network\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector_size = 50\n",
    "hidden_units = 10\n",
    "num_memory_passes = 3\n",
    "num_of_samples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_facts(facts_output):\n",
    "    \"\"\"Input: facts_output, is a 3D tensor of all the timesteps/samples. Shape=(num samples, max # words, 50)\n",
    "       Output: facts_tensor_list, a tensor list of the facts for each sample. Each 2D tensor has different shape. \"\"\"\n",
    "    facts_tensor_list = []\n",
    "    \n",
    "    for sample_index in range(num_of_samples): #iterate over each sample\n",
    "        facts = tf.nn.embedding_lookup(facts_output[sample_index], ids=sent_end_indeces[sample_index][0])\n",
    "        facts_tensor_list.append(facts)\n",
    "    \n",
    "    return facts_tensor_list\n",
    "\n",
    "    \n",
    "def get_attention(facts, question, memory):\n",
    "    \"\"\"Input: facts, 2D Tensor of the facts for each sample. question, 1D tensor of the question. memory, 1D tensor of memory.\n",
    "       Output: attentions, 1D tensor of attention scores (scalars). Implimentation as in https://arxiv.org/pdf/1603.01417.pdf\"\"\"\n",
    "    def compute_z(fact):\n",
    "        z = [tf.multiply(fact ,question), tf.multiply(fact, memory), K.abs(fact-question), K.abs(fact-memory)]\n",
    "        return K.concatenate(z, axis=0) # get an array of length 4*hidden_units.\n",
    "    \n",
    "    Zs = K.map_fn(fn=compute_z, elems=facts) # for each fact, compute z(c_t, m, q).\n",
    "    \n",
    "    g_t_i = Dense(units=word_vector_size, activation='tanh')(Zs)\n",
    "    g_t_i = Dense(units=1, activation=\"sigmoid\")(g_t_i)\n",
    "    \n",
    "    return g_t_i\n",
    "\n",
    "def semantic_module_print(sample_index, memory_iteration, facts, attentions, episode, memory):\n",
    "    print(\"Sample iter %d Memory iter %d\" % (sample_index, memory_iteration))\n",
    "    print(\"Shape of facts tensor:\", facts.shape)\n",
    "    print(\"Shape of attentions tensor:\", attentions.shape)\n",
    "    print(\"Shape of AttentionGRU input tensor:\", attention_gru_input.shape)\n",
    "    print(\"Shape of episode tensor:\", episode.shape)\n",
    "    print(\"Shape of memory tensor:\", memory.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input Module input shape=[rows, timesteps, columns/features] or [num of samples, Max context len, word vector size]\n",
    "context_input = Input(shape=(max_context_len, word_vector_size))\n",
    "context_mask = Masking(mask_value=0.0)(context_input)\n",
    "facts_output = GRU(units=hidden_units, return_sequences=True)(context_mask) # returns hidden states for all words\n",
    "facts_tensor_list = get_facts(facts_output) # extract hidden states corresponding to the 'facts'.\n",
    "\n",
    "input_model = Model(inputs=context_input, outputs=facts_output)\n",
    "input_module_outputs = input_model.predict(get_padded_contexts([x for x in range(num_of_samples)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question Module input shape=[rows, timesteps, columns/features] or [num of samples, Max # of words in q, word vector size]\n",
    "question_input = Input(shape=(max_question_len, word_vector_size))\n",
    "question_mask = Masking(mask_value=0.0)(question_input)\n",
    "question_output = GRU(units=hidden_units)(question_mask)\n",
    "\n",
    "question_model = Model(inputs=question_input, outputs=question_output)\n",
    "question_model_outputs = question_model.predict(padded_questions[0:num_of_samples].reshape(num_of_samples,-1,word_vector_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Memory Module/Answer Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we iterate over each sample in our training. For each sample we perform another for loop, which represents the Semantic Memory Module. We compute attentions for each fact of the sample, as well as the episode and finally output the last 'memory' of the final memory iteration step, this tensor we feed into the Answer Module. The Answer Module is defined in the outer for loop following on from the inner for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sample_index in range(num_of_samples): # generally this should be a list of indices of a randomized test split.\n",
    "    memory=question_output[sample_index]\n",
    "    \n",
    "    for memory_iteration in range(num_memory_passes):\n",
    "        facts, question = (facts_tensor_list[sample_index], question_output[sample_index])\n",
    "\n",
    "        attentions = get_attention(facts, question, memory)\n",
    "        attention_gru_input = tf.reshape(K.concatenate([facts, attentions], axis=1), shape=(1, -1, hidden_units+1))\n",
    "        episode = AttentionGRU(units=hidden_units)(attention_gru_input)\n",
    "        \n",
    "        memory_input=tf.expand_dims(K.concatenate([memory, tf.squeeze(episode), question], axis=0), 0) # returns 2D Tensor.\n",
    "        memory = Dense(units=hidden_units, activation='relu')(memory_input) # returns 2D tensor of shape (1, hidden_units)\n",
    "        memory=tf.squeeze(memory) # reshape from (1, hidden_units) -> (hidden_units, )\n",
    "        \n",
    "    \n",
    "    answer_length=len(answers_tokenized[sample_index]) # number of tokens we are trying to predict\n",
    "    embedded_context=string_to_embedding(contexts[sample_index])\n",
    "   \n",
    "    #a_0=tf.expand_dims(memory, axis=0) # 1D Tensor of shape (hidden units, ) -> 2D Tensor of shape (1, hidden_units)\n",
    "    #y_0=Dense(units=hidden_units, activation=\"softmax\")(a_0) # 2D Tensor of shape (1, hidden_units)\n",
    "    #a_t=GRU()(\"concats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
