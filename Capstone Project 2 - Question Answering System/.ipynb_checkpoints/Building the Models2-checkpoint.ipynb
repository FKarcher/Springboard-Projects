{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, Masking, Lambda, Bidirectional, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from EpisodicMemoryModule import EpisodicMemoryModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter/TO DO Notes\n",
    "- Word vector size (50,100,200,300)\n",
    "- Still need to find a way feed in all of the sentences into input module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Training and Dev Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_all = pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/training_data.json\").reset_index(drop=True)\n",
    "dev_df_all = pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/validation_data.json\").reset_index(drop=True)\n",
    "\n",
    "# Dataframes consisting only of 'possible' to answer questions.\n",
    "train_df = train_df_all[train_df_all.is_impossible==False][[\"context\",\n",
    "                                \"question\", \"answer_text\", \"answer_start\", \"title\"]].reset_index(drop=True)\n",
    "\n",
    "dev_df = dev_df_all[dev_df_all.is_impossible==False][[\"context\",\n",
    "                                \"question\", \"answer_text\", \"answer_start\", \"title\"]].reset_index(drop=True)\n",
    "\n",
    "train_df.answer_start = train_df.answer_start.astype(int)\n",
    "dev_df.answer_start = dev_df.answer_start.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization\n",
    "To be able to use words, phrases, questions or other natural language constructs in our model we require a to provide our neural network a numerical representation of our words (as these are the elemental NLP 'particles'). The simplest implementation would be to use 'one hot encoding' and define each word as a vector the size of our dictionary (the number of unique words found in our collection of documents, our corpus). However, this approach will most likely be insufficient for the purposes of a question answering system. word2vec and GloVe are 2 popular choices sophisticated options for word embeddings that also capture word similarities. I will not go into the details of either architecture other than to say that we will not be re-training the word vectors due to the insufficient size of the dataset, and we will begin with the GloVe word embeddings due to it's superior performance in most 'downstream' modelling tasks. Having said that, given the simplicity of swapping word vector representations we will also test out performance with word2vec (providing we can do so in a time-efficient manner).\n",
    "\n",
    "Info and download links for GloVe can be found at: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_vector_dict(url_or_path):\n",
    "    \"\"\"Takes a URL or a local path and returns a dictionary of GloVe word vectors where the key is the word and the value is the \n",
    "    word vector with the dimension specified in the input file.\"\"\"\n",
    "    with open(url_or_path, encoding=\"utf8\") as glove_text:\n",
    "        word_embeddings = [line.split(\" \") for line in glove_text.readlines()]\n",
    "    word_vector_dict = {element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "    \n",
    "    return word_vector_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector_50_dict = get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.50d.txt\")\n",
    "vocab = np.array(word_vector_50_dict.keys()) #400k words as per the documentation.\n",
    "word_vector_100_dict = get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.100d.txt\")\n",
    "\n",
    "#word_vector_200_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.200d.txt\")\n",
    "#word_vector_300_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector_size = 50\n",
    "hidden_units = 50\n",
    "input_h_units = int(hidden_units/2)\n",
    "num_memory_passes = 3\n",
    "num_of_samples = len(list(dev_df.context.values))\n",
    "dropout = 0.2\n",
    "\n",
    "num_of_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "num_of_batches = int(num_of_samples/batch_size) + 1\n",
    "# add Regularization, batch size arguments (perhaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Embedding, Padding Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def compute_everything(input_array): JUST IN CASE\\n     Takes a list of numpy question strings and returns useful stuff.\\n    Input: input_array, a numpy array of question or answer strings.\\n    Output: (tokenized_input, embedded_input, padded_input, max_input_len) a tuple of numpy arrays.\\n    \\n    tokenized_input = [tokenize(string) for string in input_array]\\n    max_input_len = np.max([len(tokens) for tokens in tokenized_input])\\n    embedded_input = [get_embedding(tokenized_input) for tokenized_input in tokenized_input]\\n    padded_input = pad_sequences(embedded_input, max_input_len*word_vector_size,\\n                          padding=\"post\", dtype=\"float32\").reshape(len(input_array), -1, word_vector_size)\\n    \\n    return (tokenized_input, embedded_input, padded_input, max_input_len)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(string):\n",
    "    \"\"\"Takes a string (either context, question or answer) and returns a list of tokens.\n",
    "    Input: string, a str object.\n",
    "    Output: a list of tokens, where each token is a substring.\"\"\"\n",
    "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in word_tokenize(string)]\n",
    "    split_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        split_tokens.extend(re.split('(\\W+)', token))\n",
    "    \n",
    "    return [token for token in split_tokens if token!=\" \" and token!=\"\"]\n",
    "\n",
    "\n",
    "def get_embedding(tokens, word_vector_dict=word_vector_50_dict):\n",
    "    \"\"\"Takes a tokenized context, question or answer and returns its vectorized/embedded form.\n",
    "    Input: tokens, list of tokens of a string.\n",
    "    Output: embedding, a numpy array of the vectorized/embedded representation of the string.\"\"\"\n",
    "    tokens = np.array(tokens)\n",
    "    embedding=[]\n",
    "        \n",
    "    for token in tokens:\n",
    "        if token in word_vector_dict.keys(): \n",
    "            embedding.extend(word_vector_dict[token])\n",
    "        else:\n",
    "            # We are vectorizing words with no embedding with the 'unk' token vectorization (already in GloVe)\n",
    "            embedding.extend(word_vector_dict[\"unk\"])\n",
    "    \n",
    "    return np.array(embedding)\n",
    "\n",
    "\n",
    "def get_sent_end_idx(context_tokenizations):\n",
    "    \"\"\" Get indices of tokens that are '.' (sentence end tokens). For one or many contexts.\n",
    "    Input: context_tokenizations, a list or numpy array of 1 or more tokenized contexts.\n",
    "    Output: a numpy array of indices of sentence end tokens.\"\"\"\n",
    "    return np.array([np.where(np.array(context)==\".\") for context in context_tokenizations])\n",
    "\n",
    "\n",
    "def compute_everything(tokenized_input, string_type=\"context\"):\n",
    "    \"\"\" Takes a numpy array of tokenized inputs and returns embedding and padding.\n",
    "    Input: input_array, a numpy array of question or answer tokenizations.\n",
    "    Output: (embedded_input, padded_input) a tuple of numpy arrays.\"\"\"\n",
    "    \n",
    "    embedded_input = [get_embedding(tokenized_input) for tokenized_input in tokenized_input]\n",
    "    if string_type==\"context\":\n",
    "        padded_input = pad_sequences(embedded_input, max_context_len*word_vector_size,\n",
    "                            padding=\"post\", dtype=\"float32\").reshape(len(tokenized_input), -1, word_vector_size)\n",
    "    elif string_type==\"question\":\n",
    "        padded_input = pad_sequences(embedded_input, max_question_len*word_vector_size,\n",
    "                            padding=\"post\", dtype=\"float32\").reshape(len(tokenized_input), -1, word_vector_size)\n",
    "    elif string_type==\"answer\":\n",
    "        padded_input = pad_sequences(embedded_input, max_answer_len*word_vector_size,\n",
    "                            padding=\"post\", dtype=\"float32\").reshape(len(tokenized_input), -1, word_vector_size)\n",
    "    \n",
    "    return (embedded_input, padded_input)\n",
    "\n",
    "def get_answer_span(answer_start, answer_end):\n",
    "    \"\"\"A function that returns one hot numpy matrices for the answer_start and answer_end indices.\n",
    "       Input: answer_start, numpy array array contained the answer start index in the context\n",
    "              answer_end, numpy array array contained the answer end index in the context\n",
    "       Output: tuple of size 2, containing the one hot embeddings of the indices for each context\"\"\"\n",
    "    y_answer_start, y_answer_end= ([] , [])\n",
    "\n",
    "    for sample_idx in range(len(answer_start)):\n",
    "        start_arr = np.zeros(shape=(max_num_chars,), dtype=float)\n",
    "        end_arr = np.zeros(shape=(max_num_chars,), dtype=float)\n",
    "        start_arr[answer_start[sample_idx]]=1.0\n",
    "        end_arr[answer_end[sample_idx]]=1.0\n",
    "        y_answer_start.append(start_arr)\n",
    "        y_answer_end.append(end_arr)\n",
    "\n",
    "    return (np.array(y_answer_start), np.array(y_answer_end))   \n",
    "\n",
    "\n",
    "\"\"\"def compute_everything(input_array): JUST IN CASE\n",
    "     Takes a list of numpy question strings and returns useful stuff.\n",
    "    Input: input_array, a numpy array of question or answer strings.\n",
    "    Output: (tokenized_input, embedded_input, padded_input, max_input_len) a tuple of numpy arrays.\n",
    "    \n",
    "    tokenized_input = [tokenize(string) for string in input_array]\n",
    "    max_input_len = np.max([len(tokens) for tokens in tokenized_input])\n",
    "    embedded_input = [get_embedding(tokenized_input) for tokenized_input in tokenized_input]\n",
    "    padded_input = pad_sequences(embedded_input, max_input_len*word_vector_size,\n",
    "                          padding=\"post\", dtype=\"float32\").reshape(len(input_array), -1, word_vector_size)\n",
    "    \n",
    "    return (tokenized_input, embedded_input, padded_input, max_input_len)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk tokenizer generated around 110,00 unique tokens from our contexts, questions and answers in our dataset. ~31,000 of those tokens did not have pre-trained word vectorizations in the GloVe model. Some of these tokens were numbers, expressed as strings in an unfamiliar format, some of these tokens were misspelled words, some of these tokens were works in other languages, or symbols from other alphibets and so on. \n",
    "With the 'regex inspired' split in the tokenized function, we were able to reduce the number of words with no embeddings to around 16,000. To deal with the remaining words with no embeddings we assigned to them the embedding for the token *'unk'*, which by definition is the embedding for unknown words provided by GloVe. Thus any word/token that did not have an embedding got an *'unk'* embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing, Embedding and Padding Contexts, Questions and Answers\n",
    "In this section we seperate out the contexts, questions and answers, we embed all our words into \n",
    "vector representations and pad the sequences to fulfil Keras' input requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contexts, questions, answers = (train_df.context.values, train_df.question.values, train_df.answer_text.values)\\nunique_contexts = train_df.context.unique()\\n\\n# Context Stuff (we embed and pad unique contexts and not all, due to memory limits)\\ncontexts_tokenized = np.array([tokenize(context) for context in contexts])\\nunique_contexts_tokenized = np.array([tokenize(context) for context in unique_contexts])\\nmax_context_len = np.max([len(context) for context in unique_contexts_tokenized])\\nunique_embedded_contexts = [get_embedding(tokenized_context) for tokenized_context in unique_contexts_tokenized] \\nunique_padded_contexts = pad_sequences(unique_embedded_contexts, max_context_len*word_vector_size,\\n                            padding=\"post\", dtype=\"f4\").reshape(len(unique_contexts), -1, word_vector_size)\\n\\n# Questions Stuff\\nmax_question_len, padded_questions = get_questions_stuff(questions)\\n\\n# Answers Stuff\\ntokenized_answers, embedded_answers, max_question_len = get_answer_stuff(answers)\\n\\n# Other useful variables\\nsent_end_indeces = get_sent_end_idx(contexts_tokenized)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"contexts, questions, answers = (train_df.context.values, train_df.question.values, train_df.answer_text.values)\n",
    "unique_contexts = train_df.context.unique()\n",
    "\n",
    "# Context Stuff (we embed and pad unique contexts and not all, due to memory limits)\n",
    "contexts_tokenized = np.array([tokenize(context) for context in contexts])\n",
    "unique_contexts_tokenized = np.array([tokenize(context) for context in unique_contexts])\n",
    "max_context_len = np.max([len(context) for context in unique_contexts_tokenized])\n",
    "unique_embedded_contexts = [get_embedding(tokenized_context) for tokenized_context in unique_contexts_tokenized] \n",
    "unique_padded_contexts = pad_sequences(unique_embedded_contexts, max_context_len*word_vector_size,\n",
    "                            padding=\"post\", dtype=\"f4\").reshape(len(unique_contexts), -1, word_vector_size)\n",
    "\n",
    "# Questions Stuff\n",
    "max_question_len, padded_questions = get_questions_stuff(questions)\n",
    "\n",
    "# Answers Stuff\n",
    "tokenized_answers, embedded_answers, max_question_len = get_answer_stuff(answers)\n",
    "\n",
    "# Other useful variables\n",
    "sent_end_indeces = get_sent_end_idx(contexts_tokenized)\n",
    "\"\"\"#padded_contexts_full=get_padded_contexts(train_df.index) Still can't fix the memory issue\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_answer_start, y_answer_end = get_answer_span(answer_start, answer_end)\\n\\n#put all this in a loop\\n# Context Stuff\\ntokenized_contexts, embedded_contexts, padded_contexts, max_context_len = compute_everything(contexts)\\n\\n# Questions Stuff\\ntokenized_questions, embedded_questions, padded_questions, max_question_len = compute_everything(questions)\\n\\n# Answers Stuff\\ntokenized_answers, embedded_answers, _, max_answer_len = compute_everything(answers)\\n\\n# Other useful variables\\nsent_end_indeces = get_sent_end_idx(tokenized_contexts)\\nanswer_lengths=[len(tokenized_answer) for tokenized_answer in tokenized_answers]\\nidx_of_max_context_len = np.where(np.array([len(context) for context in contexts])==max_context_len)[0]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the 'trans-batch' variables\n",
    "contexts, questions, answers = (dev_df.context.values, dev_df.question.values, dev_df.answer_text.values)\n",
    "answer_start = dev_df.answer_start.values\n",
    "answer_end = np.array([answer_start[idx] + len(answers[idx]) for idx in range(len(contexts))])\n",
    "\n",
    "tokenized_contexts = [tokenize(context) for context in contexts]\n",
    "tokenized_questions = [tokenize(question) for question in questions]\n",
    "tokenized_answers = [tokenize(answer) for answer in answers]\n",
    "\n",
    "# Calculating variables used within the model architecture\n",
    "max_context_len = np.max([len(context) for context in tokenized_contexts])\n",
    "max_question_len = np.max([len(question) for question in tokenized_questions])\n",
    "max_answer_len = np.max([len(answer) for answer in tokenized_answers])\n",
    "\n",
    "# The length of the output array (longest context length, in number of characters in the string)\n",
    "sent_end_indeces = get_sent_end_idx(tokenized_contexts)\n",
    "max_num_chars = np.max([len(context) for context in contexts]) #must use between batches\n",
    "batch_iteration = 0\n",
    "\"\"\"y_answer_start, y_answer_end = get_answer_span(answer_start, answer_end)\n",
    "\n",
    "#put all this in a loop\n",
    "# Context Stuff\n",
    "tokenized_contexts, embedded_contexts, padded_contexts, max_context_len = compute_everything(contexts)\n",
    "\n",
    "# Questions Stuff\n",
    "tokenized_questions, embedded_questions, padded_questions, max_question_len = compute_everything(questions)\n",
    "\n",
    "# Answers Stuff\n",
    "tokenized_answers, embedded_answers, _, max_answer_len = compute_everything(answers)\n",
    "\n",
    "# Other useful variables\n",
    "sent_end_indeces = get_sent_end_idx(tokenized_contexts)\n",
    "answer_lengths=[len(tokenized_answer) for tokenized_answer in tokenized_answers]\n",
    "idx_of_max_context_len = np.where(np.array([len(context) for context in contexts])==max_context_len)[0]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Dynamic Memory Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_facts(facts_output):\n",
    "    \"\"\" Extracts the timesteps (facts) for each sample, then pads each tensor. Returns a 3D tensor.\n",
    "        Input: facts_output, is a 3D tensor of all the timesteps/samples. Shape=(num samples, max # words, 50)\n",
    "        Output: facts_tensor_list, a tensor list of the facts for each sample. Each 2D tensor has different shape. \"\"\"\n",
    "    facts_tensor_list = []\n",
    "    start_idx = batch_iteration*batch_size\n",
    "    \n",
    "    for sample_index in range(facts_output.shape[0]): #iterate over each sample in the batch. instead of batch_size .......\n",
    "        facts = tf.nn.embedding_lookup(facts_output[sample_index], ids=sent_end_indeces[start_idx+sample_index][0])\n",
    "        facts_tensor_list.append(facts)\n",
    "    \n",
    "    max_num_facts = np.max([facts_tensor.shape[0] for facts_tensor in facts_tensor_list])\n",
    "    \n",
    "    # Padding the tensors\n",
    "    for idx in range(len(facts_tensor_list)):\n",
    "        numpy_pad = np.zeros(shape=(max_num_facts-facts_tensor_list[idx].shape[0], hidden_units))\n",
    "        pad = tf.Variable(numpy_pad, dtype=tf.float32)\n",
    "        facts_tensor_list[idx] = K.concatenate([facts_tensor_list[idx], pad], axis=0)\n",
    "    \n",
    "    return tf.stack(facts_tensor_list) # list of 2D tensors -> 3D tensor of shape (num samples, max_num_facts, hidden_units)\n",
    "\n",
    "\n",
    "def get_batch_data(batch_size=batch_size, num_of_batches=num_of_batches): \n",
    "    \n",
    "    for batch_idx in range(num_of_batches): #this will return stuff for each batch (once we put it into function)\n",
    "        \"\"\" Get start and end indices of the batch \"\"\"\n",
    "        print(\"Batch iteration:\", batch_iteration)\n",
    "        print(\"Batch idx:\", batch_idx)\n",
    "        \n",
    "        start = batch_idx*batch_size\n",
    "        if (batch_idx+1)*batch_size < num_of_samples: # if our ending index is not out of bounds\n",
    "            end = (batch_idx+1)*batch_size\n",
    "        else: # if our ending index is out of bounds, we set it to the last index\n",
    "            end = num_of_samples-1\n",
    "        \n",
    "        \"\"\" Compute semi-useful variables \"\"\"\n",
    "        answer_lengths=[len(tokenized_answer) for tokenized_answer in tokenized_answers[start:end]]\n",
    "        idx_of_max_context_len = np.where(np.array([len(context) for context in contexts[start:end]])==max_context_len)[0]\n",
    "    \n",
    "        yield [get_answer_span(answer_start[start:end], answer_end[start:end]), #returns y_start, y_end\n",
    "               compute_everything(tokenized_contexts[start:end], string_type=\"context\"), \n",
    "               compute_everything(tokenized_questions[start:end], string_type=\"question\"),\n",
    "               compute_everything(tokenized_answers[start:end], string_type=\"answer\"),\n",
    "               answer_lengths, idx_of_max_context_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What it does:** The Input Module iterates over each context (paragraph of a wikipedia article) and returns embedded representations (facts) of each sentence in the context (for each sample).\n",
    "- **How it does this:** The Input Module uses a Bidirectional GRU that iterates over each word and returns the hidden state after each iteration. The GRU requires the input to be a 3D Tensor of shape (samples, timesteps, columns/features), and each sample must have the same (timesteps, columns/features) shape. However, given that our contexts are of different length (different amounts of words/timesteps), the embedded representations are padded (have 0.0s appended to each context embedding) as to meet the input requirements. As a technical side note, all embedded contexts are padded such that their length is equal to the length of the longest context in the whole sample (again, such that each individual sample has the same shape). To ensure that the GRU interprets the 0.0s as paddings, we mask the input (via the Masking layer). A Dropout Layer is added as a form of regularization. The *get_facts* method then returns the facts for each sample/context by extracting the hidden states/outputs of the GRU corresponding to 'end of sentence token' timesteps (exactly as described in the *'Ask Me Anything...'* paper.\n",
    "\n",
    "**Input Module input shape:** *[rows, timesteps, columns/features]* or *[num of samples, max # words in context (*max_context_len*), word vector size]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start idx is: 0\n",
      "Start idx is: 0\n"
     ]
    }
   ],
   "source": [
    "context_input = Input(shape=(max_context_len, word_vector_size), name=\"ContextInput\")\n",
    "context_mask = Masking(mask_value=0.0, name=\"ContextMask\")(context_input)\n",
    "facts_output = Bidirectional(GRU(units=input_h_units, return_sequences=True),\n",
    "                             merge_mode=\"concat\", name=\"ContextBiGRU\")(context_mask)\n",
    "facts_output = Dropout(dropout, name=\"ContextBiGRU_Dropout\")(facts_output)\n",
    "facts_tensors = Lambda(get_facts, name=\"FactTensorLambda\")(facts_output)\n",
    "facts_mask = Masking(mask_value=0.0, name=\"FactTensorMask\")(facts_tensors)\n",
    "\n",
    "#input_model = Model(inputs=context_input, outputs=facts_mask)\n",
    "#print(input_model.summary())\n",
    "#input_module_outputs = input_model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What it does:** The Question Module (similarly to the input module) returns an embedded representation for each question. However, unlike the Input Module, the only output of the Question Module is the last hidden state of the GRU (the embedded representation). This again, is exactly the implementation defined in the beforementioned paper, but it makes sense, considering that all questions are 1 sentence long.\n",
    "- **How it does it:** Similarly to the input module (spotting a trend here) the input has to be padded and masked since not all questions are of equal length. This time a 'vanilla' unidirectional GRU is used as there is only 1 sentence/sequence, and so we wouldn't benefit from a bibirectional architecture like in the Input Module. Given that the outputs of the GRU are exactly what we want, no further processing is required.\n",
    "\n",
    "**Question Module input shape:** *[rows, timesteps, columns/features]* or *[num of samples, max # words in question (max_question_len), word vector size]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_input = Input(shape=(max_question_len, word_vector_size), name=\"QuestionInput\")\n",
    "question_mask = Masking(mask_value=0.0, name=\"QuestionMask\")(question_input)\n",
    "question_output = GRU(units=hidden_units, name=\"QuestionGRU\")(question_mask)\n",
    "question_output = Dropout(dropout, name=\"QuestionOutputDropout\")(question_output)\n",
    "\n",
    "#question_model = Model(inputs=question_input, outputs=question_output)\n",
    "#question_model_outputs = question_model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Memory Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ContextInput (InputLayer)       (None, 708, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ContextMask (Masking)           (None, 708, 50)      0           ContextInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "QuestionInput (InputLayer)      (None, 36, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ContextBiGRU (Bidirectional)    (None, 708, 50)      11400       ContextMask[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "QuestionMask (Masking)          (None, 36, 50)       0           QuestionInput[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ContextBiGRU_Dropout (Dropout)  (None, 708, 50)      0           ContextBiGRU[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "QuestionGRU (GRU)               (None, 50)           15150       QuestionMask[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "FactTensorLambda (Lambda)       (100, 10, 50)        0           ContextBiGRU_Dropout[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "QuestionOutputDropout (Dropout) (None, 50)           0           QuestionGRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "episodic_memory_module_2 (Episo (None, 100)          0           FactTensorLambda[0][0]           \n",
      "                                                                 QuestionOutputDropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "FinalMemoryDropout (Dropout)    (None, 100)          0           episodic_memory_module_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "StartIdxProbs (Dense)           (None, 4063)         410363      FinalMemoryDropout[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "EndIdxProbs (Dense)             (None, 4063)         410363      FinalMemoryDropout[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 847,276\n",
      "Trainable params: 847,276\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get this to accept Context Masking Layer\n",
    "epm_output = EpisodicMemoryModule(units=hidden_units, batch_size=batch_size, emb_dim=word_vector_size,\n",
    "            memory_steps=num_memory_passes)([facts_tensors, question_output])\n",
    "epm_output = Dropout(dropout, name=\"FinalMemoryDropout\")(epm_output) # 1 m^Tm for 1 sample.\n",
    "\n",
    "# Calculates probabilities for each sample, that the start or index is at some location. expects 2D input.\n",
    "start_idx_probs = Dense(units=max_num_chars, activation=\"softmax\",\n",
    "                        name=\"StartIdxProbs\")(epm_output) #shape (num samples, max_context_len)\n",
    "end_idx_probs = Dense(units=max_num_chars, activation=\"softmax\",\n",
    "                     name=\"EndIdxProbs\")(epm_output) #shape (num samples, max_context_len)\n",
    "\n",
    "DMN_model = Model(inputs=[context_input, question_input], outputs=[start_idx_probs, end_idx_probs])\n",
    "DMN_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "print(DMN_model.summary())\n",
    "\n",
    "#DMN_model.fit(x=[padded_contexts, padded_questions], y=[y_answer_start, y_answer_end],\n",
    "              #epochs=10, batch_size=batch_size, validation_split=0.1)\n",
    "\n",
    "#answers=Lambda(get_answer)(epm_output)  \n",
    "#answers = get_answer(epm_output)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch iteration: 0\n",
      "Batch idx: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas Buteliauskas\\Anaconda3_\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [100] vs. [32]\n\t [[Node: training_1/SGD/gradients/loss_1/EndIdxProbs_loss/mul_1_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@training_1/SGD/gradients/loss_1/EndIdxProbs_loss/mul_1_grad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_1/SGD/gradients/loss_1/EndIdxProbs_loss/mul_1_grad/Shape, training_1/SGD/gradients/loss_1/EndIdxProbs_loss/mul_1_grad/Shape_1)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1d4e696c0b1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0manswers_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0midx_of_max_context_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mDMN_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpadded_contexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_questions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_answer_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_answer_end\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mbatch_iteration\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[1;32m-> 1454\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [100] vs. [32]\n\t [[Node: training_1/SGD/gradients/loss_1/EndIdxProbs_loss/mul_1_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@training_1/SGD/gradients/loss_1/EndIdxProbs_loss/mul_1_grad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_1/SGD/gradients/loss_1/EndIdxProbs_loss/mul_1_grad/Shape, training_1/SGD/gradients/loss_1/EndIdxProbs_loss/mul_1_grad/Shape_1)]]"
     ]
    }
   ],
   "source": [
    "all_batch_data = get_batch_data()\n",
    "\n",
    "for batch_idx in range(num_of_batches):\n",
    "    batch_data = all_batch_data.__next__() # Getting next value in the generator (next batch)\n",
    "    y_answer_start, y_answer_end = batch_data[0]\n",
    "    embedded_contexts, padded_contexts = batch_data[1]\n",
    "    embedded_questions, padded_questions = batch_data[2]\n",
    "    embedded_answers, padded_answers = batch_data[3]\n",
    "    answers_lengths, idx_of_max_context_len = (batch_data[4], batch_data[5])\n",
    "    \n",
    "    \n",
    "    history = DMN_model.fit(x=[padded_contexts, padded_questions],\n",
    "                            y=[y_answer_start, y_answer_end], validation_split=0.1)\n",
    "    batch_iteration+=1\n",
    "\n",
    "batch_iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_samples%38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"def get_answer(memory_embeddings):\n",
    "    Takes a 2D Tensor of final memory outputs, and returns answer embeddings.\n",
    "       Input: memory_embeddings, 2D Tensor, Episodic Memory Module output, final memories for each sample.\n",
    "    all_predictions = []\n",
    "    for sample_idx in range(int(memory_embeddings.shape[0])):\n",
    "        question, memory=(tf.expand_dims(question_output[sample_idx], axis=0), memory_embeddings[sample_idx])\n",
    "        sample_predictions = []\n",
    "        a_0 = tf.expand_dims(memory, axis=0) \n",
    "        y_last = 0\n",
    "        a_t = 0\n",
    "        print(\"Sample\", sample_idx)\n",
    "        \n",
    "        for answer_index in range(answer_lengths[sample_idx]):  # iterate answer_length times\n",
    "            if answer_index==0:\n",
    "                y_last = Dense(units=word_vector_size, activation=\"softmax\")(a_0) # initial prediction\n",
    "                sample_predictions.append(y_last) # appending first word\n",
    "            else:\n",
    "                gru_input = tf.reshape(tf.stack(K.concatenate([y_last, question], axis=1)), shape=[1, 1, -1])\n",
    "                a_t = GRU(units=hidden_units)(gru_input)\n",
    "                y_last = Dense(units=word_vector_size, activation=\"softmax\")(a_t) # prediction\n",
    "                sample_predictions.append(y_last)\n",
    "            print(\"Answer %d/%d\" % (answer_index+1, answer_lengths[sample_idx]))\n",
    "            \n",
    "        all_predictions.append(sample_predictions)\n",
    "    \n",
    "    \n",
    "    return all_predictions[0][0]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def get_attention(facts, question, memory):\n",
    "    Input: facts, 2D Tensor of the facts for each sample. question, 1D tensor of the question. memory, 1D tensor of memory.\n",
    "       Output: attentions, 1D tensor of attention scores (scalars). Implimentation as in https://arxiv.org/pdf/1603.01417.pdf\n",
    "    def compute_z(fact):\n",
    "        z = [tf.multiply(fact ,question), tf.multiply(fact, memory), K.abs(fact-question), K.abs(fact-memory)]\n",
    "        return K.concatenate(z, axis=0) # get an array of length 4*hidden_units.\n",
    "    \n",
    "    Zs = K.map_fn(fn=compute_z, elems=facts) # for each fact, compute z(c_t, m, q).\n",
    "    \n",
    "    g_t_i = Dense(units=word_vector_size, activation='tanh')(Zs)\n",
    "    g_t_i = Dense(units=1, activation=\"sigmoid\")(g_t_i)\n",
    "    \n",
    "    return g_t_i\n",
    "\n",
    "def semantic_module_print(sample_index, memory_iteration, facts, attentions, episode, memory):\n",
    "     Some test prints \n",
    "    print(\"Sample iter %d Memory iter %d\" % (sample_index, memory_iteration))\n",
    "    print(\"Shape of facts tensor:\", facts.shape)\n",
    "    print(\"Shape of attentions tensor:\", attentions.shape)\n",
    "    print(\"Shape of Attention GRU input tensor:\", attention_gru_input.shape)\n",
    "    print(\"Shape of episode tensor:\", episode.shape)\n",
    "    print(\"Shape of memory tensor:\", memory.shape, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for sample_index in range(num_of_samples): # generally this should iterate over batches\n",
    "    facts, question= (facts_tensor_list[sample_index], question_output[sample_index])\n",
    "    memory=question # the initial memory is set to be the question\n",
    "    answer_length = len(tokenized_answers[sample_index]) # number of tokens we are trying to predict\n",
    "    sample_predictions=[]\n",
    "    \n",
    "    for memory_iteration in range(num_memory_passes): # Episodic Memory Module\n",
    "        attentions = get_attention(facts, question, memory)\n",
    "        attention_gru_input = tf.reshape(K.concatenate([facts, attentions], axis=1), shape=(1, -1, hidden_units+1))\n",
    "        episode = AttentionGRU(units=hidden_units)(attention_gru_input)\n",
    "        episode = Dropout(dropout)(episode)\n",
    "        \n",
    "        memory_input=tf.expand_dims(K.concatenate([memory, tf.squeeze(episode), question], axis=0), 0) # returns 2D Tensor.\n",
    "        memory = Dense(units=hidden_units, activation='relu')(memory_input) # returns 2D tensor of shape (1, hidden_units)\n",
    "        memory = tf.squeeze(memory) # reshape from (1, hidden_units) -> (hidden_units, )\n",
    "        _ = semantic_module_print(sample_index, memory_iteration, facts, attentions, episode, memory)\n",
    "    \n",
    "    for answer_index in range(answer_length): # predict answer_length word embeddings for each sample\n",
    "        a_0=tf.expand_dims(memory, axis=0) # shape (hidden units, ) -> (1, hidden_units)\n",
    "        y_last=0\n",
    "        a_t=0\n",
    "        \n",
    "        if answer_index==0:\n",
    "            y_last=Dense(units=word_vector_size, activation=\"softmax\")(a_0) # prediction \n",
    "            sample_predictions.append(y_last)\n",
    "        else:\n",
    "            a_t=GRU(units=hidden_units, initial_states=a_0)(K.concatenate([y_last, question], axis=1))\n",
    "            y_last=Dense(units=word_vector_size, activation=\"softmax\")(a_t) # prediction \n",
    "            sample_predictions.append(y_last)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
