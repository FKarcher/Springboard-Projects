{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, Masking, Lambda, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from attention_gru import SoftAttnGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter/TO DO Notes\n",
    "- Word vector size (50,100,200,300)\n",
    "- Still need to find a way feed in all of the sentences into input module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/training_data.json\").reset_index(drop=True)\n",
    "dev_df=pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/validation_data.json\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization\n",
    "To be able to use words, phrases, questions or other natural language constructs in our model we require a to provide our neural network a numerical representation of our words (as these are the elemental NLP 'particles'). The simplest implementation would be to use 'one hot encoding' and define each word as a vector the size of our dictionary (the number of unique words found in our collection of documents, our corpus). However, this approach will most likely be insufficient for the purposes of a question answering system. word2vec and GloVe are 2 popular choices sophisticated options for word embeddings that also capture word similarities. I will not go into the details of either architecture other than to say that we will not be re-training the word vectors due to the insufficient size of the dataset, and we will begin with the GloVe word embeddings due to it's superior performance in most 'downstream' modelling tasks. Having said that, given the simplicity of swapping word vector representations we will also test out performance with word2vec (providing we can do so in a time-efficient manner).\n",
    "\n",
    "Info and download links for GloVe can be found at: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Word Vector' Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_vector_dict(url_or_path):\n",
    "    \"\"\"Takes a URL or a local path and returns a dictionary of GloVe word vectors where the key is the word and the value is the \n",
    "    word vector with the dimension specified in the input file.\"\"\"\n",
    "    with open(url_or_path, encoding=\"utf8\") as glove_text:\n",
    "        word_embeddings=[line.split(\" \") for line in glove_text.readlines()]\n",
    "    word_vector_dict={element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "    \n",
    "    return word_vector_dict\n",
    "\n",
    "\n",
    "def get_word_vector_df(url_path_or_dict):\n",
    "    \"\"\"Takes a URL or path like the previous function, or can take a word vector dictionary and returns a word vector dataframe.\n",
    "    Rows of the dataframe are the word vectors, columns are the dimensions of the word vector, indices are the words.\"\"\"\n",
    "    \n",
    "    if type(url_path_or_dict) is str:\n",
    "        with open(url_path_or_dict, encoding=\"utf8\") as glove_text:\n",
    "            word_lines=glove_text.readlines()\n",
    "        word_embeddings=[line.split(\" \") for line in word_lines]\n",
    "        word_vector_dict={element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "        word_vector_df=pd.DataFrame(word_vector_dict).transpose()\n",
    "    \n",
    "    else:\n",
    "        word_vector_df=pd.DataFrame(url_path_or_dict).transpose()\n",
    "    \n",
    "    return word_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Word Vectors\n",
    "As mentioned above with regards to what model we use for the word vectors, it's important to note that the dimention of the word vectors is a hyperparameter of the Neural Networks to come, so to keep our options open we imported a few different word vectors representations and the custom functions defined above make this a 'one line of code' affair (dictionary or dataframe).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector_50_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.50d.txt\")\n",
    "word_vector_50_df=get_word_vector_df(word_vector_50_dict)\n",
    "vocab=np.array(word_vector_50_dict.keys()) #400k words as per the documentation.\n",
    "\n",
    "word_vector_100_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.100d.txt\")\n",
    "word_vector_100_df=get_word_vector_df(word_vector_100_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_vector_200_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.200d.txt\")\\nword_vector_200_df=get_word_vector_df(word_vector_200_dict)\\n\\nword_vector_300_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.300d.txt\")\\nword_vector_300_df=get_word_vector_df(word_vector_300_dict)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"word_vector_200_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.200d.txt\")\n",
    "word_vector_200_df=get_word_vector_df(word_vector_200_dict)\n",
    "\n",
    "word_vector_300_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.300d.txt\")\n",
    "word_vector_300_df=get_word_vector_df(word_vector_300_dict)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Embedding Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    \"\"\"Takes a string (either context, question or answer) and returns a list of tokens.\"\"\"\n",
    "    tokens=[token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in word_tokenize(string)]\n",
    "    \n",
    "    split_tokens=[]\n",
    "    for token in tokens:\n",
    "        split_tokens.extend(re.split('(\\W+)', token))\n",
    "    return [token for token in split_tokens if token!=\" \" and token!=\"\"]\n",
    "\n",
    "\n",
    "def string_to_embedding(string, word_vector_dict=word_vector_50_dict):\n",
    "    \"\"\"Takes a context, question or answer and returns a list of word tokens in their vectorized form.\n",
    "    Updated to accept tokenizations for faster training.\"\"\"\n",
    "    tokens=[]\n",
    "    embedding=[]\n",
    "    \n",
    "    if isinstance(string, str) is True:\n",
    "        tokens=np.array(tokenize(string))\n",
    "    else:\n",
    "        tokens=np.array(string)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in word_vector_dict.keys():\n",
    "            embedding.extend(word_vector_dict[token])\n",
    "        else:\n",
    "            # We are vectorizing words with no embedding with the 'unk' token vectorization (already in GloVe)\n",
    "            embedding.extend(word_vector_dict[\"unk\"])\n",
    "    \n",
    "    return np.array(embedding)\n",
    "\n",
    "\n",
    "def get_sent_end_idx(context_tokenizations):\n",
    "    \"\"\" Get indices of tokens that are '.' (sentence end tokens). For one or many contexts.\"\"\"\n",
    "    return np.array([np.where(np.array(context)==\".\") for context in context_tokenizations])\n",
    "\n",
    "\n",
    "def get_padded_contexts(dataframe_indices_or_contexts):\n",
    "    \"\"\"Take a list of indices or a list of contexts and return a list of padded context embeddings ready for the Input Module.\"\"\"\n",
    "    context_index_pairs=list(zip(train_df.context.values, train_df.index.values))\n",
    "    padded_contexts_fresh=[]\n",
    "    \n",
    "    if isinstance(dataframe_indices_or_contexts, int): # if a single index is provided\n",
    "        context=context_index_pairs[dataframe_indices_or_contexts][0]\n",
    "        padding_index=int(np.where(unique_contexts==context)[0])\n",
    "        padded_contexts_fresh.append(unique_padded_contexts[padding_index])\n",
    "        \n",
    "    elif isinstance(dataframe_indices_or_contexts[0], int): # if we are given a list of indices.\n",
    "          for index in dataframe_indices_or_contexts:\n",
    "                context=context_index_pairs[index][0]\n",
    "                padding_index=int(np.where(unique_contexts==context)[0])\n",
    "                padded_contexts_fresh.append(unique_padded_contexts[padding_index])\n",
    "    \n",
    "    elif isinstance(dataframe_indices_or_contexts[0], str): # if a list of contexts or a single context is provided\n",
    "        if len(dataframe_indices_or_contexts[0])!=1: # if a list of contexts\n",
    "            for context in dataframe_indices_or_contexts:\n",
    "                padding_index=int(np.where(unique_contexts==context)[0])\n",
    "                padded_contexts_fresh.append(unique_padded_contexts[padding_index])   \n",
    "        else: # if a single context\n",
    "            padding_index=int(np.where(unique_contexts==dataframe_indices_or_contexts)[0])\n",
    "            padded_contexts_fresh.append(unique_padded_contexts[padding_index])\n",
    "    \n",
    "    else:\n",
    "        print(\"NO EMBEDDING PROVIDED, INCORRECT PARAMETER DATA TYPE\")\n",
    "      \n",
    "    return np.array(padded_contexts_fresh).reshape(len(padded_contexts_fresh),-1,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk tokenizer generated around 110,00 unique tokens from our contexts, questions and answers in our dataset. ~31,000 of those tokens did not have pre-trained word vectorizations in the GloVe model. Some of these tokens were numbers, expressed as strings in an unfamiliar format, some of these tokens were misspelled words, some of these tokens were works in other languages, or symbols from other alphibets and so on. \n",
    "The ideal case would be to assign to each token in our corpus a 'meaningful' word vectorization, however given the time constraint on this project, we will try to provide meaningful word vectorizations to as many of these tokens as possible through pre-processing steps following the initial word tokenization (using the nltk tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing, Embedding and Padding Contexts, Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#padded_contexts_full=get_padded_contexts(train_df.index) Still can't fix the memory issue\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"In this section we seperate out the contexts, questions and answers, we embed finally embed all our words into \n",
    "vector representations and pad the sequences to fulfil Keras' input requirements.\"\"\"\n",
    "\n",
    "# Context Stuff (we embed and pad unique contexts and not all, due to memory limits)\n",
    "contexts, questions, answers=(train_df.context.values, train_df.question.values, train_df.answer_text.values)\n",
    "contexts_tokenized=np.array([tokenize(context) for context in contexts])\n",
    "#embedded_contexts=[string_to_embedding(tokenized_context) for tokenized_context in contexts_tokenized]\n",
    "unique_contexts=train_df.context.unique()\n",
    "unique_contexts_tokenized=np.array([tokenize(context) for context in unique_contexts])\n",
    "max_context_len=np.max([len(context) for context in unique_contexts_tokenized])\n",
    "unique_embedded_contexts=[string_to_embedding(tokenized_context) for tokenized_context in unique_contexts_tokenized]  \n",
    "unique_padded_contexts=pad_sequences(unique_embedded_contexts, max_context_len*50, padding=\"post\",\n",
    "                              dtype=np.dtype('f4')).reshape(len(unique_contexts),-1,50)\n",
    "\n",
    "\n",
    "# Questions Stuff\n",
    "questions_tokenized=np.array([tokenize(question) for question in questions])\n",
    "max_question_len=np.max([len(question) for question in questions_tokenized])\n",
    "embedded_questions=np.array([string_to_embedding(tokenized_question) for tokenized_question in questions_tokenized])\n",
    "padded_questions=pad_sequences(embedded_questions, max_question_len*50, padding=\"post\",\n",
    "                               dtype=\"float32\").reshape(len(questions),-1,50)\n",
    "\n",
    "\n",
    "# Answers Stuff\n",
    "answers_tokenized=np.array([tokenize(answer) for answer in answers])\n",
    "embedded_answers=np.array([string_to_embedding(tokenized_answer) for tokenized_answer in answers_tokenized])\n",
    "max_answer_len=np.max([len(answer) for answer in answers_tokenized])\n",
    "\n",
    "# Other useful variables\n",
    "sent_end_indeces=get_sent_end_idx(contexts_tokenized)\n",
    "sentence_lengths=sentence_lengths=[len(indices[0]) for indices in sent_end_indeces]\n",
    "max_num_sentences=np.max(sentence_lengths)\n",
    "\"\"\"#padded_contexts_full=get_padded_contexts(train_df.index) Still can't fix the memory issue\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dynamic Memory Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                \"\"\" Hyperparameters\"\"\"\n",
    "word_vector_size=50\n",
    "hidden_units=10\n",
    "num_memory_passes=3\n",
    "num_of_samples=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'indices': sent_end_indeces[0:num_of_samples]} # WILL NEED TO CHANGE THIS LATER\n",
    "setattr(K, 'params', params)\n",
    "sent_end_indeces = K.params['indices']\n",
    "\n",
    "def get_fact_representations(input_module_output, sent_end_indeces):\n",
    "    \"\"\" We take in an N-Dim Array of shape (num samples, number of timesteps/words, size of fact embedding) and return the \n",
    "    facts of each sample (context) at the timesteps corresponding to 'ends of sentences' (at '.').\"\"\"\n",
    "    \"\"\" Do not mistake the method parameter for the array of sent_end_indeces. The sent_end_indeces here is a subset of the\n",
    "    original array.\"\"\"\n",
    "    assert input_module_output.shape[0]==sent_end_indeces.shape[0] #number of samples is equal to the given number of indices.\n",
    "    complete_fact_representations=[]\n",
    "    \n",
    "    if input_module_output.shape[0]!=1: # for a list of samples\n",
    "        for idx, timesteps in enumerate(input_module_output):\n",
    "            sample_fact_representations=timesteps[sent_end_indeces[idx][0]] # get the timesteps at the end of sentences\n",
    "            complete_fact_representations.append(sample_fact_representations)\n",
    "    else: # for a single sample\n",
    "         complete_fact_representations.append(input_module_output[0][sent_end_indeces[0]])\n",
    "    \n",
    "    return np.array(complete_fact_representations)\n",
    "\n",
    "\n",
    "def get_facts(facts_output):\n",
    "    \"\"\"Input: facts_output, is a 3D tensor of all the timesteps/samples. Shape=(num samples, max # words, 50)\n",
    "       Output: facts_tensor_list, a tensor list of the facts for each sample. Each 2D tensor has different shape. \"\"\"\n",
    "    facts_tensor_list=[]\n",
    "    \n",
    "    for sample_index in range(num_of_samples): #iterate over each sample\n",
    "        facts=tf.nn.embedding_lookup(facts_output[sample_index], ids=sent_end_indeces[sample_index][0])\n",
    "        facts_tensor_list.append(facts)\n",
    "    \n",
    "    return facts_tensor_list\n",
    "\n",
    "\n",
    "def compute_attention(facts, question, memory):\n",
    "    \"\"\"Input: facts, 2D Tensor of the facts for each sample. question, 1D tensor of the question. memory, 1D tensor of memory.\n",
    "       Output: attentions, 1D tensor of attention scores (scalars)\"\"\"\n",
    "    def get_single_attention(fact):\n",
    "        z=[tf.multiply(fact, question), tf.multiply(fact, memory), K.abs(fact-question), K.abs(fact-memory)]\n",
    "        z_new=Lambda((lambda x: K.stack(x)))([K.concatenate(z, axis=0)])\n",
    "        g_t_i = Dense(units=word_vector_size, activation='tanh')(K.concatenate(z, axis=0)) # LOOK HERE BRANKO. shape=(40,)\n",
    "        #g_t_i = Dense(units=word_vector_size, activation='tanh')(z_new) has shape (1,40)\n",
    "        g_t_i = Dense(units=1, activation=\"sigmoid\")(g_t_i)\n",
    "        return g_t_i\n",
    "    \n",
    "    attentions=tf.map_fn(fn=get_single_attention, elems=facts)\n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context_input=Input(shape=(max_context_len, word_vector_size))\n",
    "context_mask=Masking(mask_value=0.0)(context_input)\n",
    "facts_output=GRU(units=hidden_units, return_sequences=True)(context_mask)\n",
    "facts_tensor_list=get_facts(facts_output)\n",
    "\n",
    "input_model=Model(inputs=context_input, outputs=facts_output)\n",
    "input_module_outputs=input_model.predict(get_padded_contexts([x for x in range(num_of_samples)]))\n",
    "#facts_collection_numpy=get_fact_representations(input_module_outputs, sent_end_indeces[0:num_of_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question module input shape=[rows, timesteps, columns] or [num of samples, Max # of words in q, word vector size]\n",
    "\n",
    "question_input=Input(shape=(max_question_len, word_vector_size))\n",
    "question_mask=Masking(mask_value=0.0)(question_input)\n",
    "question_output=GRU(units=hidden_units)(question_mask)\n",
    "\n",
    "question_model=Model(inputs=question_input, outputs=question_output)\n",
    "question_model_outputs=question_model.predict(padded_questions[0:num_of_samples].reshape(num_of_samples,-1,word_vector_size))\n",
    "\n",
    "#EVERYTHING HERE IS FINE, USE QUESTION_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "(1, 40)\n"
     ]
    }
   ],
   "source": [
    "                                                    \"\"\" TESTING \"\"\"\n",
    "\"\"\"This is a simulation of the error happening in the 'compute_attention' method. The dense layer expects a 2D tensor. Is \"\"\"\n",
    "facts=facts_tensor_list[0]\n",
    "print(K.concatenate([facts[0], facts[1], facts[2], facts[3]], axis=0).shape)\n",
    "print(K.stack([K.concatenate([facts[0], facts[1], facts[2], facts[3]])], axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample iter 0 Memory iter 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer dense_19: expected min_ndim=2, found ndim=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-841bb3e48839>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;34m\"\"\"Error is in the line below\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mattentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfacts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Get attention scores for each fact in a sample.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-120-7a69c489f4b7>\u001b[0m in \u001b[0;36mcompute_attention\u001b[1;34m(facts, question, memory)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mg_t_i\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mattentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_single_attention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfacts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mattentions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36mmap_fn\u001b[1;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         swap_memory=swap_memory)\n\u001b[0m\u001b[0;32m    424\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\u001b[0m\n\u001b[0;32m   3222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3223\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3224\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3225\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3226\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2955\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 2956\u001b[1;33m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   2957\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2958\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2891\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m   2892\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2893\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2894\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2895\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(i, tas)\u001b[0m\n\u001b[0;32m    411\u001b[0m       \"\"\"\n\u001b[0;32m    412\u001b[0m       \u001b[0mpacked_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem_ta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem_ta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0melems_ta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m       \u001b[0mpacked_fn_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m       \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpacked_fn_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m       \u001b[0mflat_fn_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_fn_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-120-7a69c489f4b7>\u001b[0m in \u001b[0;36mget_single_attention\u001b[1;34m(fact)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mz_new\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mg_t_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_vector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# LOOK HERE BRANKO. shape=(40,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;31m#g_t_i = Dense(units=word_vector_size, activation='tanh')(z_new) has shape (1,40)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mg_t_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_t_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    325\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected min_ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    328\u001b[0m             \u001b[1;31m# Check dtype.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer dense_19: expected min_ndim=2, found ndim=1"
     ]
    }
   ],
   "source": [
    "                                        \"\"\" Semantic Memory Module \"\"\"\n",
    "for sample_index in range(num_of_samples): # Iterating over each sample index (we can't iterate over context-question)\n",
    "    memory=question_output[sample_index]\n",
    "    \n",
    "    for memory_iteration in range(num_memory_passes):\n",
    "        facts=facts_tensor_list[sample_index]\n",
    "        question=question_output[sample_index]\n",
    "        print(\"Sample iter %d Memory iter %d\" %(sample_index, memory_iteration))\n",
    "        \n",
    "        \"\"\"Error is in the line below\"\"\"\n",
    "        attentions=compute_attention(facts, question , memory) # Get attention scores for each fact in a sample.\n",
    "    \n",
    "    \n",
    "        # new_memory=GRU(units=hidden_units)(e^i)\n",
    "        #memory=new_memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def get_unique_tokens(df):\n",
    "    #Given a dataframe containing contexts, questions and answers, the function returns a list of unique tokens.\n",
    "    pieces_of_text=list(df[\"context\"].unique()) + list(df[\"title\"].unique()) + list(df[\"question\"].unique()) \n",
    "    pieces_of_text+=list(df[\"answer_text\"].unique())\n",
    "\n",
    "    non_unique_tokens=[]\n",
    "    for text in pieces_of_text:\n",
    "        temp_tokens=word_tokenize(text)\n",
    "        non_unique_tokens.append(temp_tokens)\n",
    "\n",
    "    unique_tokens=set()\n",
    "    for token in non_unique_tokens:\n",
    "        unique_tokens.update(set(token))\n",
    "    \n",
    "    return [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in list(unique_tokens)]\n",
    "\n",
    "def split_keep_sep(tokens, sep, only_unique=True):\n",
    "    Takes a string or a list of tokens, and splits on 'sep' while keeping sep. Returns a set of unique tokens, or a list\n",
    "    of tokens after splitting.\n",
    "    DO NOT PASS IN A STRING OR IT WILL RETURN A BUNCH OF CHARACTERS\n",
    "    split_tokens=[]\n",
    "    for token in tokens:\n",
    "        if only_unique==True:\n",
    "            for sub_token in re.split(\"(\"+ sep + \")\", token):\n",
    "                if sub_token not in split_tokens:\n",
    "                    split_tokens.append(sub_token)\n",
    "        else: # if we want just a list of all the tokens after seperation\n",
    "            split_tokens.extend(re.split(\"(\"+ sep + \")\", token))\n",
    "                \n",
    "    return [token for token in split_tokens if token!=\"\"]\"\"\"\n",
    "\n",
    "\"\"\"def tokenize_further(tokens, seperator_list=[\"-\",\"–\",\"—\",\"'\",\"£\",\"/\",\":\"]):\n",
    "    Further splits the tokens on the seperators, while keeping the seperators as tokens.\n",
    "    Returns a further tokenized list of tokens.\n",
    "    split_tokens=split_keep_sep(tokens, seperator_list[0], only_unique=False)\n",
    "    for sep in seperator_list[1:]:\n",
    "        split_tokens=split_keep_sep(split_tokens, sep, only_unique=False)\n",
    "    \n",
    "    return [token for token in split_tokens if token!=\"\"] # filters out \"\" (empty space tokens)\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "unique_tokens=get_unique_tokens(train_df)\n",
    "no_embeddings=[token for token in unique_tokens if token not in word_vector_50_dict.keys()]\n",
    "split_unique_tokens=split_keep_sep(unique_tokens, \"-\")\n",
    "split_unique_tokens2=split_keep_sep(split_unique_tokens, \"–\")\n",
    "split_unique_tokens3=split_keep_sep(split_unique_tokens2, \"—\")\n",
    "split_unique_tokens4=split_keep_sep(split_unique_tokens3, \"'\")\n",
    "split_unique_tokens5=split_keep_sep(split_unique_tokens4, \"£\")\n",
    "split_unique_tokens6=split_keep_sep(split_unique_tokens5, \"/\")\n",
    "split_unique_tokens7=split_keep_sep(split_unique_tokens6, \":\")\n",
    "\n",
    "print(\"Num of initial tokens:\", len(unique_tokens), \"\\nNum of tokens after '-' split:\", len(split_unique_tokens))\n",
    "print(\"Num of tokens after 2nd split\", len(split_unique_tokens2),\"\\nNum of tokens after 3rd split\", len(split_unique_tokens3))\n",
    "print(\"Num of tokens after 4th split\", len(split_unique_tokens4), \"\\nNum of tokens after 5th split\", len(split_unique_tokens5))\n",
    "print(\"Num of tokens after 6th split\", len(split_unique_tokens6), \"\\nNum of tokens after 7th split\", len(split_unique_tokens7),\"\\n\")\n",
    "\n",
    "no_embeddings_2=[token for token in split_unique_tokens if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_3=[token for token in split_unique_tokens2 if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_4=[token for token in split_unique_tokens3 if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_5=[token for token in split_unique_tokens4 if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_6=[token for token in split_unique_tokens5 if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_7=[token for token in split_unique_tokens6 if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_8=[token for token in split_unique_tokens7 if token not in word_vector_50_dict.keys()]\n",
    "\n",
    "print(\"Number of tokens with no embedding in GloVe (with the current tokenization):\")\n",
    "print(\"with '-' with words:\",len(no_embeddings),\"\\nwith '-' seperate:\", len(no_embeddings_2),)\n",
    "print(\"with 2 - with words:\", len(no_embeddings_3),\"\\nwith all - with words:\", len(no_embeddings_4))\n",
    "print(\"with all - and ' with words:\", len(no_embeddings_5) ,\"\\nwith all -, ', £ with words:\", len(no_embeddings_6))\n",
    "print(\"with all -, ', £, / with words:\", len(no_embeddings_7),\"\\nwith all -, ', £, /, : with words:\", len(no_embeddings_8),\"\\n\")\n",
    "\n",
    "print(no_embeddings_8[0:100])\n",
    "\n",
    "\n",
    "def string_to_embedding(string, word_vector_dict=word_vector_50_dict, context=False):\n",
    "    Takes a context, question or answer and returns a list of word tokens in their vectorized form.\n",
    "    If context, return a list of sequences. Updated to accept tokenizations for faster training.\n",
    "    tokens=[]\n",
    "    embedding=[]\n",
    "    if isinstance(string, str) is True:\n",
    "        tokens=np.array(tokenize(string))\n",
    "    else:\n",
    "        tokens=np.array(string)\n",
    "    \n",
    "    if context is False:\n",
    "        for token in tokens:\n",
    "            if token in word_vector_dict.keys():\n",
    "                embedding.extend(word_vector_dict[token])\n",
    "            else:\n",
    "                # We are vectorizing words with no embedding with the 'unk' token vectorization (already in GloVe)\n",
    "                embedding.extend(word_vector_dict[\"unk\"])\n",
    "    else:\n",
    "        #Seperate out the sentences (on the '.')\n",
    "        dot_indeces=np.nonzero(tokens==\".\")[0]\n",
    "        prev_dot=0\n",
    "        sentences=[]\n",
    "        for dot in dot_indeces:\n",
    "            sentences.append(tokens[prev_dot:dot+1])\n",
    "            prev_dot=dot+1\n",
    "    \n",
    "        #Embed the sentences     \n",
    "        for sentence in sentences:\n",
    "            sentence_embedding=[]\n",
    "            \n",
    "            for token in sentence:\n",
    "                if token in word_vector_dict.keys():\n",
    "                    sentence_embedding.extend(word_vector_dict[token])\n",
    "                else:\n",
    "                    sentence_embedding.extend(word_vector_dict[\"unk\"])\n",
    "        \n",
    "            embedding.append(sentence_embedding)\n",
    "            sentence_embedding=[]\n",
    "    \n",
    "    return np.array(embedding)\n",
    "    \n",
    "def get_triplet(df, row_idx, cols=[\"context\", \"question\", \"answer_text\"]):\n",
    "    #Takes a dataframe and row index, and returns a (context, question, answer) triplet as a tuple.\n",
    "    triplet_list=train_df.loc[row_idx, cols].values\n",
    "    return (triplet_list[0], triplet_list[1], triplet_list[2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "context_index_dict=defaultdict(list)\n",
    "context_index_pairs=list(zip(train_df.context.values, train_df.index.values))\n",
    "for context, index in context_index_pairs:\n",
    "    context_index_dict[context].append(index)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_z(c_t,m,q):\n",
    "     Takes a facts array (c_t) for a single sample, a memory (m^i-1) and the question (q) and computes a 'similarity array'.\n",
    "    Implimentation as in https://arxiv.org/pdf/1603.01417.pdf \n",
    "    z_s=[]\n",
    "    for c in c_t: #length of each z is 4*len(c,m,q)\n",
    "        z=[]\n",
    "        z.extend(np.multiply(c,q))\n",
    "        z.extend(np.multiply(c,m))\n",
    "        z.extend(np.absolute(c-q))\n",
    "        z.extend(np.absolute(c-m))\n",
    "        z_s.append(z)\n",
    "    return np.array(z_s)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def get_fact_representations_new(input_module_output):\n",
    "     We take in an N-Dim Array of shape (num samples, number of timesteps/words, size of fact embedding) and return the \n",
    "    facts of each sample (context) at the timesteps corresponding to 'ends of sentences' (at '.').\"\"\"\n",
    "    \"\"\" Do not mistake the method parameter for the array of sent_end_indeces. The sent_end_indeces here is a subset of the\n",
    "    original array.\n",
    "    sent_end_indeces = K.params['indices']\n",
    "    facts_tensor=input_module_output\n",
    "    #assert input_module_output.shape[0]==sent_end_indeces.shape[0] #number of samples is equal to the given number of indices.\n",
    "    \n",
    "    if facts_tensor.shape[0]!=1: # for a list of samples\n",
    "        for idx, timesteps in enumerate(facts_tensor):\n",
    "            timesteps=timesteps[sent_end_indeces[idx][0]] # get the timesteps at the end of sentences\n",
    "    else: # for a single sample\n",
    "        print(\"Reached else\")\n",
    "        print(\"facts_tensor[0]\", facts_tensor[0])\n",
    "        print(\"facts_tensor[0] shape:\", facts_tensor[0].shape)\n",
    "        print(\"sent_end_indeces[0]:\", sent_end_indeces[0])\n",
    "        facts_tensor=facts_tensor[0][sent_end_indeces[0]]\n",
    "    \n",
    "\n",
    "    return facts_tensor\n",
    "\n",
    "\n",
    "def get_facts(facts_output):\n",
    "    Input: inputs is a 3D tensor of all the timesteps/samples. Shape=(num samples, max # words, 50)\n",
    "       Output: 3D tensor of the facts for each sample (context). Each 2D tensor has different shape. \n",
    "    sent_end_indeces = K.params['indices']\n",
    "    #assert input_module_output.shape[0]==sent_end_indeces.shape[0] #number of samples is equal to the given number of indices.\n",
    "    sample_idx=0\n",
    "    def extract_facts(sample_timesteps):\n",
    "        #facts=sample_timesteps[sent_end_indeces[sample_idx][0]] #extract timesteps corresponding to <EOS> token, \".\". \n",
    "        facts=sample_timesteps[sent_end_indeces[0][0]] #extract timesteps corresponding to <EOS> token, \".\". \n",
    "        #print(\"Sample idx:\", sample_idx)\n",
    "        print(\"Inside\")\n",
    "        print(sample_timesteps.shape)\n",
    "        #sample_idx+=1\n",
    "        return sample_timesteps\n",
    "    \n",
    "    print(\"Sample idx:\", sample_idx)\n",
    "    facts_tensor=K.map_fn(fn=extract_facts, elems=facts_output) \n",
    "    \n",
    "    return facts_tensor\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_z_tensors(c_t,m,q):\n",
    "     Input: facts array (c_t) for a SINGLE SAMPLE, Memory (m^i-1), Question (q) and computes a 'similarity tensors'.\n",
    "        Output: z(c_t,m,q) evaluated for each fact, so z_s len is number of facts c_t for that context.\n",
    "        Implimentation as in https://arxiv.org/pdf/1603.01417.pdf \n",
    "    \n",
    "    def compute_z(c_t): # 'tensor function' to compute z array for each fact.\n",
    "        return K.concatenate([tf.multiply(c_t,q), tf.multiply(c_t,m), K.abs(c_t-q), K.abs(c_t-m)], axis=0)\n",
    "    \n",
    "    z_s=K.map_fn(fn=compute_z, elems=c_t) #for each fact, compute z(c_t, m, q). This is basically the tensor 'for loop'.\n",
    "    print(\"z_s of a single sample:\",z_s)\n",
    "    \n",
    "    return z_s\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_facts_(facts_output):\n",
    "    MAYBE WORK ON THIS TO GET A 3D TENSOR OUTPUT\n",
    "    sent_end_indeces = K.params['indices']\n",
    "    sample_index=0\n",
    "        \n",
    "    def extract_facts(sample_timesteps): \n",
    "        facts=tf.nn.embedding_lookup(facts_output[sample_index], ids=indices)\n",
    "        sample_idx+=1\n",
    "        return facts\n",
    "    \n",
    "    print(\"Sample idx:\", sample_idx)\n",
    "    facts_tensor=K.map_fn(fn=extract_facts, elems=facts_output) \n",
    "       \n",
    "    return facts_tensor\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
