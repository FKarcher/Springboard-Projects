{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Notes\n",
    "-Word vector size (50,100,200,300)\n",
    "-how to deal with tokens with no word vectorization (REMOVE, generate random array, spell checker?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/training_data.json\").reset_index(drop=True)\n",
    "dev_df=pd.read_json(\"C:/Users/Lukas Buteliauskas/Desktop/validation_data.json\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorization\n",
    "To be able to use words, phrases, questions or other natural language constructs in our model we require a to provide our neural network a numerical representation of our words (as these are the elemental NLP 'particles'). The simplest implementation would be to use 'one hot encoding' and define each word as a vector the size of our dictionary (the number of unique words found in our collection of documents, our corpus). However, this approach will most likely be insufficient for the purposes of a question answering system. word2vec and GloVe are 2 popular choices sophisticated options for word embeddings that also capture word similarities. I will not go into the details of either architecture other than to say that we will not be re-training the word vectors due to the insufficient size of the dataset, and we will begin with the GloVe word embeddings due to it's superior performance in most 'downstream' modelling tasks. Having said that, given the simplicity of swapping word vector representations we will also test out performance with word2vec (providing we can do so in a time-efficient manner).\n",
    "\n",
    "Info and download links for GloVe can be found at: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Word Vector' Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector_dict(url_or_path):\n",
    "    \"\"\"Takes a URL or a local path and returns a dictionary of GloVe word vectors where the key is the word and the value is the \n",
    "    word vector with the dimension specified in the input file.\"\"\"\n",
    "    \n",
    "    with open(url_or_path, encoding=\"utf8\") as glove_text:\n",
    "        word_lines=glove_text.readlines()\n",
    "    word_embeddings=[line.split(\" \") for line in word_lines]\n",
    "    word_vector_dict={element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "    \n",
    "    return word_vector_dict\n",
    "\n",
    "\n",
    "def get_word_vector_df(url_path_or_dict):\n",
    "    \"\"\"Takes a URL or path like the previous function, or can take a word vector dictionary and returns a word vector dataframe.\n",
    "    Rows of the dataframe are the word vectors, columns are the dimensions of the word vector, indices are the words.\"\"\"\n",
    "    \n",
    "    if type(url_path_or_dict) is str:\n",
    "        with open(url_path_or_dict, encoding=\"utf8\") as glove_text:\n",
    "            word_lines=glove_text.readlines()\n",
    "        word_embeddings=[line.split(\" \") for line in word_lines]\n",
    "        word_vector_dict={element[0]:list(map(float, element[1:])) for element in word_embeddings}\n",
    "        word_vector_df=pd.DataFrame(word_vector_dict).transpose()\n",
    "    \n",
    "    else:\n",
    "        word_vector_df=pd.DataFrame(url_path_or_dict).transpose()\n",
    "    \n",
    "    return word_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Word Vectors\n",
    "As mentioned above with regards to what model we use for the word vectors, it's important to note that the dimention of the word vectors is a hyperparameter of the Neural Networks to come, so to keep our options open we imported a few different word vectors representations and the custom functions defined above make this a 'one line of code' affair (dictionary or dataframe).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector_50_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.50d.txt\")\n",
    "word_vector_50_df=get_word_vector_df(word_vector_50_dict)\n",
    "vocab=np.array(word_vector_50_dict.keys()) #400k words as per the documentation.\n",
    "\n",
    "word_vector_100_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.100d.txt\")\n",
    "word_vector_100_df=get_word_vector_df(word_vector_100_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_vector_200_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.200d.txt\")\\nword_vector_200_df=get_word_vector_df(word_vector_200_dict)\\n\\nword_vector_300_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.300d.txt\")\\nword_vector_300_df=get_word_vector_df(word_vector_300_dict)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"word_vector_200_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.200d.txt\")\n",
    "word_vector_200_df=get_word_vector_df(word_vector_200_dict)\n",
    "\n",
    "word_vector_300_dict=get_word_vector_dict(\"C:/Users/Lukas Buteliauskas/Desktop/glove.6B.300d.txt\")\n",
    "word_vector_300_df=get_word_vector_df(word_vector_300_dict)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring all tokens have word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Embedding Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_tokens(df):\n",
    "    \"\"\"Given a dataframe containing contexts, questions and answers, the function returns a list of unique tokens.\"\"\"\n",
    "    pieces_of_text=list(df[\"context\"].unique()) + list(df[\"title\"].unique()) + list(df[\"question\"].unique()) \n",
    "    pieces_of_text+=list(df[\"answer_text\"].unique())\n",
    "\n",
    "    non_unique_tokens=[]\n",
    "    for text in pieces_of_text:\n",
    "        temp_tokens=word_tokenize(text)\n",
    "        non_unique_tokens.append(temp_tokens)\n",
    "\n",
    "    unique_tokens=set()\n",
    "    for token in non_unique_tokens:\n",
    "        unique_tokens.update(set(token))\n",
    "    \n",
    "    return [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in list(unique_tokens)]\n",
    "\n",
    "\n",
    "def split_keep_sep(tokens, sep, only_unique=True):\n",
    "    \"\"\"Takes a string or a list of tokens, and splits on 'sep' while keeping sep. Returns a set of unique tokens, or a list\n",
    "    of tokens after splitting.\n",
    "    DO NOT PASS IN A STRING OR IT WILL RETURN A BUNCH OF CHARACTERS\"\"\"\n",
    "    split_tokens=[]\n",
    "    for token in tokens:\n",
    "        if only_unique==True:\n",
    "            for sub_token in re.split(\"(\"+ sep + \")\", token):\n",
    "                if sub_token not in split_tokens:\n",
    "                    split_tokens.append(sub_token)\n",
    "        else: # if we want just a list of all the tokens after seperation\n",
    "            split_tokens.extend(re.split(\"(\"+ sep + \")\", token))\n",
    "                \n",
    "    return [token for token in split_tokens if token!=\"\"]\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\"Takes a string (either context, question or answer) and returns a list of tokens.\"\"\"\n",
    "    return [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in word_tokenize(string)]\n",
    "\n",
    "\n",
    "def tokenize_further(tokens, seperator_list=[\"-\",\"–\",\"—\",\"'\",\"£\",\"/\",\":\"]):\n",
    "    \"\"\"Further splits the tokens on the seperators, while keeping the seperators as tokens.\n",
    "    Returns a further tokenized list of tokens.\"\"\"\n",
    "    split_tokens=split_keep_sep(tokens, seperator_list[0], only_unique=False)\n",
    "    for sep in seperator_list[1:]:\n",
    "        split_tokens=split_keep_sep(split_tokens, sep, only_unique=False)\n",
    "    \n",
    "    return [token for token in split_tokens if token!=\"\"] # filters out \"\" (empty space tokens)\n",
    "\n",
    "def string_to_embedding(string, word_vector_dict=word_vector_50_dict):\n",
    "    \"\"\"Takes a context, question or answer and returns a list of word tokens in their vectorized form.\"\"\"\n",
    "    tokens=tokenize_further(tokenize(string))\n",
    "    embedding=[]  \n",
    "    \n",
    "    #Go over each token in the string (after all the preprocessing)\n",
    "    for token in tokens:\n",
    "        if token in word_vector_dict.keys(): # if it has a vectorization, add it to the 'embedding' list.\n",
    "            embedding.extend(word_vector_dict[token])\n",
    "        else:\n",
    "            \"\"\"Apply function to generate random vectorization. CURRENTLY WE ARE IGNORING WORDS THAT DON'T HAVE AN EMBEDDING.\"\"\"\n",
    "            \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk tokenizer generated around 110,00 unique tokens from our contexts, questions and answers in our dataset. ~31,000 of those tokens did not have pre-trained word vectorizations in the GloVe model. Some of these tokens were numbers, expressed as strings in an unfamiliar format, some of these tokens were misspelled words, some of these tokens were works in other languages, or symbols from other alphibets and so on. \n",
    "The ideal case would be to assign to each token in our corpus a 'meaningful' word vectorization, however given the time constraint on this project, we will try to provide meaningful word vectorizations to as many of these tokens as possible through pre-processing steps following the initial word tokenization (using the nltk tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_tokens=get_unique_tokens(train_df)\n",
    "no_embeddings=[token for token in unique_tokens if token not in word_vector_50_dict.keys()]\n",
    "split_unique_tokens=split_keep_sep(unique_tokens, \"-\")\n",
    "split_unique_tokens2=split_keep_sep(split_unique_tokens, \"–\")\n",
    "split_unique_tokens3=split_keep_sep(split_unique_tokens2, \"—\")\n",
    "split_unique_tokens4=split_keep_sep(split_unique_tokens3, \"'\")\n",
    "split_unique_tokens5=split_keep_sep(split_unique_tokens4, \"£\")\n",
    "split_unique_tokens6=split_keep_sep(split_unique_tokens5, \"/\")\n",
    "split_unique_tokens7=split_keep_sep(split_unique_tokens6, \":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of initial tokens: 119319 \n",
      "Num of tokens after '-' split: 94470\n",
      "Num of tokens after 2nd split 92798 \n",
      "Num of tokens after 3rd split 91669\n",
      "Num of tokens after 4th split 90768 \n",
      "Num of tokens after 5th split 90639\n",
      "Num of tokens after 6th split 89796 \n",
      "Num of tokens after 7th split 89386 \n",
      "\n",
      "Number of tokens with no embedding in GloVe (with the current tokenization):\n",
      "with '-' with words: 31521 \n",
      "with '-' seperate: 24311\n",
      "with 2 - with words: 22541 \n",
      "with all - with words: 21396\n",
      "with all - and ' with words: 20560 \n",
      "with all -, ', £ with words: 20395\n",
      "with all -, ', £, / with words: 19604 \n",
      "with all -, ', £, /, : with words: 19385 \n",
      "\n",
      "['ummah.com', 'kumārila', 'neighbourhods', 'makhtesh', 'hernandia', 'infrastructure.', '27,799', 'zŏnglĭ', 'whitsand', 'danishnamah', 'stsc', 'iātros', 'asace', 'chaetae', 'g.m.c', '1,530,000', 'paedobaptism', 'khiuma', 'indvuna', 'catepillars', 'finges', 'blueware', 'hutecheson', 'avicide', 'zakria', 'feminity', 'kilikiti', 'linters', 'malevoncia', 'harjumaa', 'animalcules', 'parskahay', '紅包', 'abdacom', 'mendezes', 'afshona', 'ceatively', 'ˌseɪntˈdʒɒnz', 'macpublisher', 'disel', '7x7m', 'overhunts', 'πελλήνη', '牠', 'baudhanath', 'isil', 'chavurah', 'theudisk', 'qianhu', '676,578', 'ἀρχι', 'fireguards', 'data1', 'ˈplɪməθ', 'saëns', 'anem', 'automeris', '~6997100000000000000♠1×10−3', 'revoled', 'zoskales', 'haruspices', 'actof', 'newsok.com', 'nesiota', 'disputers', 'pollarding', 'henrican', 'mtdnas', 'ājīvika', 'jusice', 'broeder', 'ag3cu', 'cetdz', 'klipstein', 'polymyxins', 'dipylon', '⟨ɦ⟩', '3892', 'madraza', 'siones', 'toiny', 'aftar', 'conformities', 'bhandasar', 'adamts7', 'galatasarey', 'swro', 'kavod', 'xalwo', '*aluþ', 'bioremediate', 'yingtian', 'peruskoulu', '1,310,000', 'barruts', 'coburgs', 'adserà', 'mektebi', '€25', 'λέγειν']\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of initial tokens:\", len(unique_tokens), \"\\nNum of tokens after '-' split:\", len(split_unique_tokens))\n",
    "print(\"Num of tokens after 2nd split\", len(split_unique_tokens2),\"\\nNum of tokens after 3rd split\", len(split_unique_tokens3))\n",
    "print(\"Num of tokens after 4th split\", len(split_unique_tokens4), \"\\nNum of tokens after 5th split\", len(split_unique_tokens5))\n",
    "print(\"Num of tokens after 6th split\", len(split_unique_tokens6), \"\\nNum of tokens after 7th split\", len(split_unique_tokens7),\"\\n\")\n",
    "\n",
    "no_embeddings_2=[token for token in split_unique_tokens if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_3=[token for token in split_unique_tokens2 if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_4=[token for token in split_unique_tokens3 if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_5=[token for token in split_unique_tokens4 if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_6=[token for token in split_unique_tokens5 if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_7=[token for token in split_unique_tokens6 if token not in word_vector_50_dict.keys()]\n",
    "no_embeddings_8=[token for token in split_unique_tokens7 if token not in word_vector_50_dict.keys()]\n",
    "\n",
    "print(\"Number of tokens with no embedding in GloVe (with the current tokenization):\")\n",
    "print(\"with '-' with words:\",len(no_embeddings),\"\\nwith '-' seperate:\", len(no_embeddings_2),)\n",
    "print(\"with 2 - with words:\", len(no_embeddings_3),\"\\nwith all - with words:\", len(no_embeddings_4))\n",
    "print(\"with all - and ' with words:\", len(no_embeddings_5) ,\"\\nwith all -, ', £ with words:\", len(no_embeddings_6))\n",
    "print(\"with all -, ', £, / with words:\", len(no_embeddings_7),\"\\nwith all -, ', £, /, : with words:\", len(no_embeddings_8),\"\\n\")\n",
    "\n",
    "print(no_embeddings_8[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.73129, 0.62083, 0.04594, 0.77703, -0.5803, 0.27172, 0.28471, 0.32273, 0.8567, 1.5485, 0.54048, 0.21277, 0.13194, 0.013766, -0.15691, -0.396, -0.6855, -0.39768, 0.8253, -0.31198, -0.18205, 1.246, 0.96381, 0.048889, 0.71645, 0.82746, -1.067, -0.38175, -0.37723, -0.74626, 0.11207, 0.17772, 0.59491, -0.6364, 0.17068, -0.50995, 1.0413, -0.60245, -0.73349, -1.0289, -0.18171, -0.49433, -0.34356, -1.2125, 0.43832, -0.51488, -0.037414, -1.9207, -0.2878, 0.79805, 0.44796, 0.20505, -1.7072, 0.25522, -0.61353, 0.45521, 0.5134, 0.36352, 0.13682, -0.49604, 0.58205, 0.90981, -0.83431, 0.37364, 0.7367, 0.15918, 0.27294, 0.23022, 0.56572, -0.85467, 0.13297, 0.76733, -0.04357, -0.031591, 0.45783, 0.37597, -0.8988, 0.94555, -1.3402, -0.46339, -0.92484, 0.73547, -0.0067005, 0.68015, -0.27115, -0.50333, 0.29133, 0.13423, 0.6459, 0.21893, -0.28878, 0.58685, 0.30512, -1.0662, -0.49959, -0.60076, 0.159, 0.020639, 0.28208, 0.54983, -0.82729, 0.1195, 0.39781, 0.81701, -0.1012, 0.33862, -0.0038192, 0.21697, 0.5085, 0.48307, 0.31873, 0.63366, -0.092657, -0.0080337, -0.3484, -0.16589, -0.16129, -0.92026, 0.073372, -0.20204, 0.021461, 0.65181, 0.82864, -0.11776, 0.80457, 0.059549, -0.16814, -0.6802, -0.87874, -0.64642, 0.26874, -0.17237, 0.64114, -0.31916, 0.0077973, -0.6054, 0.76914, -0.39535, -0.40607, -0.7932, 0.068244, 0.29379, -0.51429, -1.189, 0.30639, -0.010483, 0.018851, -1.7352, -0.37566, 0.72065] \n",
      "\n",
      "[-0.73129, 0.62083, 0.04594, 0.77703, -0.5803, 0.27172, 0.28471, 0.32273, 0.8567, 1.5485, 0.54048, 0.21277, 0.13194, 0.013766, -0.15691, -0.396, -0.6855, -0.39768, 0.8253, -0.31198, -0.18205, 1.246, 0.96381, 0.048889, 0.71645, 0.82746, -1.067, -0.38175, -0.37723, -0.74626, 0.11207, 0.17772, 0.59491, -0.6364, 0.17068, -0.50995, 1.0413, -0.60245, -0.73349, -1.0289, -0.18171, -0.49433, -0.34356, -1.2125, 0.43832, -0.51488, -0.037414, -1.9207, -0.2878, 0.79805] \n",
      "\n",
      "[0.44796, 0.20505, -1.7072, 0.25522, -0.61353, 0.45521, 0.5134, 0.36352, 0.13682, -0.49604, 0.58205, 0.90981, -0.83431, 0.37364, 0.7367, 0.15918, 0.27294, 0.23022, 0.56572, -0.85467, 0.13297, 0.76733, -0.04357, -0.031591, 0.45783, 0.37597, -0.8988, 0.94555, -1.3402, -0.46339, -0.92484, 0.73547, -0.0067005, 0.68015, -0.27115, -0.50333, 0.29133, 0.13423, 0.6459, 0.21893, -0.28878, 0.58685, 0.30512, -1.0662, -0.49959, -0.60076, 0.159, 0.020639, 0.28208, 0.54983] \n",
      "\n",
      "[-0.82729, 0.1195, 0.39781, 0.81701, -0.1012, 0.33862, -0.0038192, 0.21697, 0.5085, 0.48307, 0.31873, 0.63366, -0.092657, -0.0080337, -0.3484, -0.16589, -0.16129, -0.92026, 0.073372, -0.20204, 0.021461, 0.65181, 0.82864, -0.11776, 0.80457, 0.059549, -0.16814, -0.6802, -0.87874, -0.64642, 0.26874, -0.17237, 0.64114, -0.31916, 0.0077973, -0.6054, 0.76914, -0.39535, -0.40607, -0.7932, 0.068244, 0.29379, -0.51429, -1.189, 0.30639, -0.010483, 0.018851, -1.7352, -0.37566, 0.72065] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_list=string_to_embedding(train_df.loc[0,[\"context\"]][0])\n",
    "print(embedding_list[0:150],\"\\n\")\n",
    "print(word_vector_50_dict[\"beyoncé\"],\"\\n\")\n",
    "print(word_vector_50_dict[\"giselle\"],\"\\n\")\n",
    "print(word_vector_50_dict[\"knowles\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"yon\" in word_vector_50_dict.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
