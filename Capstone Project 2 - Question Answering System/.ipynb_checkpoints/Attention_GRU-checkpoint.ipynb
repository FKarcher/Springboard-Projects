{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Attention GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro and Code Origin\n",
    "- This is an implementation of an attention based GRU as described in https://arxiv.org/pdf/1506.07285.pdf and https://arxiv.org/pdf/1603.01417.pdf. \n",
    "- The code (currently) is an exact of copy of the work done by GitHub user vchudinov in his project to implement the Dynamic Memory Network (as described in the papers listed above) and can be found at https://github.com/vchudinov/dynamic_memory_networks_with_keras. The license permits (amongst other things): personal/commercial use, distribution and modification of the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.ops import array_ops\n",
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "class SoftAttnGRU(Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 activation='tanh',\n",
    "                 recurrent_activation='hard_sigmoid',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 recurrent_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 implementation=1,\n",
    "                 return_sequences=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"Identical to keras.recurrent.GRUCell.\n",
    "        The difference comes from the computation in self.call\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        super(SoftAttnGRU, self).__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.return_sequences = return_sequences\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        self.implementation = implementation\n",
    "        self.state_size = self.units\n",
    "        self._dropout_mask = None\n",
    "        self._recurrent_dropout_mask = None\n",
    "\n",
    "        self._input_map = {}\n",
    "\n",
    "        super(SoftAttnGRU, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        out = list(input_shape)\n",
    "        out[-1] = self.units\n",
    "        if self.return_sequences:\n",
    "            return out\n",
    "        else:\n",
    "            return (out[0], out[-1])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        input_dim = input_shape[-1] - 1\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units * 3),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 3),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units * 3,),\n",
    "                                        name='bias',\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.kernel_z = self.kernel[:, :self.units]\n",
    "        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n",
    "        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n",
    "        self.recurrent_kernel_r = self.recurrent_kernel[:,\n",
    "                                                        self.units:\n",
    "                                                        self.units * 2]\n",
    "        self.kernel_h = self.kernel[:, self.units * 2:]\n",
    "        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_z = self.bias[:self.units]\n",
    "            self.bias_r = self.bias[self.units: self.units * 2]\n",
    "            self.bias_h = self.bias[self.units * 2:]\n",
    "        else:\n",
    "            self.bias_z = None\n",
    "            self.bias_r = None\n",
    "            self.bias_h = None\n",
    "        super(SoftAttnGRU, self).build(input_shape)\n",
    "\n",
    "    def step(self, inputs, states, training=None):\n",
    "        \"\"\"Computes the output of a single step. Unlike the vanilla GRU, attention is applied to the\n",
    "        output, as per https://arxiv.org/pdf/1603.01417.pdf\n",
    "        ----------\n",
    "        inputs : (K.Tensor)\n",
    "            A tensor of shape [batch_size, input_size+1]. The last element of each example is the\n",
    "            attention score.\n",
    "        states : (K.Tensor)\n",
    "            Initial (list) of states\n",
    "        training : (bool)\n",
    "            Whether the network is in training mode or not. \n",
    "        Returns\n",
    "        -------\n",
    "        (K.Tensor)\n",
    "            The output for the current step, modified by attention\n",
    "        \"\"\"\n",
    "            # Needs question as an input\n",
    "        x_i, attn_gate = array_ops.split(inputs,\n",
    "                                         num_or_size_splits=[self.units, 1], axis=1)\n",
    "        h_tm1 = states[0]\n",
    "\n",
    "        # dropout matrices for input units\n",
    "        dp_mask = self._dropout_mask\n",
    "        # dropout matrices for recurrent units\n",
    "        rec_dp_mask = self._recurrent_dropout_mask\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs_z = x_i * dp_mask[0]\n",
    "                inputs_r = x_i * dp_mask[1]\n",
    "                inputs_h = x_i * dp_mask[2]\n",
    "            else:\n",
    "                inputs_z = x_i\n",
    "                inputs_r = x_i\n",
    "                inputs_h = x_i\n",
    "            x_z = K.dot(inputs_z, self.kernel_z)\n",
    "            x_r = K.dot(inputs_r, self.kernel_r)\n",
    "            x_h = K.dot(inputs_h, self.kernel_h)\n",
    "            if self.use_bias:\n",
    "                x_z = K.bias_add(x_z, self.bias_z)\n",
    "                x_r = K.bias_add(x_r, self.bias_r)\n",
    "                x_h = K.bias_add(x_h, self.bias_h)\n",
    "\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1_z = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_r = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_h = h_tm1 * rec_dp_mask[2]\n",
    "            else:\n",
    "                h_tm1_z = h_tm1\n",
    "                h_tm1_r = h_tm1\n",
    "                h_tm1_h = h_tm1\n",
    "\n",
    "            z = self.recurrent_activation(\n",
    "                x_z + K.dot(h_tm1_z, self.recurrent_kernel_z))\n",
    "            r = self.recurrent_activation(\n",
    "                x_r + K.dot(h_tm1_r, self.recurrent_kernel_r))\n",
    "\n",
    "            hh = self.activation(x_h + K.dot(r * h_tm1_h,\n",
    "                                             self.recurrent_kernel_h))\n",
    "        else:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                x_i *= dp_mask[0]\n",
    "            matrix_x = K.dot(x_i, self.kernel)\n",
    "            if self.use_bias:\n",
    "                matrix_x = K.bias_add(matrix_x, self.bias)\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1 *= rec_dp_mask[0]\n",
    "            matrix_inner = K.dot(h_tm1,\n",
    "                                 self.recurrent_kernel[:, :2 * self.units])\n",
    "\n",
    "            x_z = matrix_x[:, :self.units]\n",
    "            x_r = matrix_x[:, self.units: 2 * self.units]\n",
    "            recurrent_z = matrix_inner[:, :self.units]\n",
    "            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n",
    "\n",
    "            z = self.recurrent_activation(x_z + recurrent_z)\n",
    "            r = self.recurrent_activation(x_r + recurrent_r)\n",
    "\n",
    "            x_h = matrix_x[:, 2 * self.units:]\n",
    "            recurrent_h = K.dot(r * h_tm1,\n",
    "                                self.recurrent_kernel[:, 2 * self.units:])\n",
    "            hh = self.activation(x_h + recurrent_h)\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "\n",
    "        # Attention modulated output.\n",
    "        h = attn_gate * h + (1 - attn_gate) * h_tm1\n",
    "\n",
    "        if 0 < self.dropout + self.recurrent_dropout:\n",
    "            if training is None:\n",
    "                h._uses_learning_phase = True\n",
    "        return h, [h]\n",
    "\n",
    "    def call(self, input_list, initial_state=None, mask=None, training=None):\n",
    "\n",
    "        inputs = input_list\n",
    "\n",
    "        self._generate_dropout_mask(inputs, training=training)\n",
    "        self._generate_recurrent_dropout_mask(inputs, training=training)\n",
    "\n",
    "        # if has_arg(self.layer.call, 'training'):\n",
    "        self.training = training\n",
    "        uses_learning_phase = False\n",
    "        initial_state = self.get_initial_state(inputs)\n",
    "\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        last_output, outputs, _ = K.rnn(self.step,\n",
    "                                        inputs=inputs,\n",
    "                                        constants=[],\n",
    "                                        initial_states=initial_state,\n",
    "                                        input_length=input_shape[1],\n",
    "                                        unroll=False)\n",
    "        if self.return_sequences:\n",
    "            y = outputs\n",
    "        else:\n",
    "            y = last_output\n",
    "\n",
    "        if (hasattr(self, 'activity_regularizer') and\n",
    "                self.activity_regularizer is not None):\n",
    "            regularization_loss = self.activity_regularizer(y)\n",
    "            self.add_loss(regularization_loss, inputs)\n",
    "\n",
    "        if uses_learning_phase:\n",
    "            y._uses_learning_phase = True\n",
    "\n",
    "        if self.return_sequences:\n",
    "            timesteps = input_shape[1]\n",
    "            new_time_steps = list(y.get_shape())\n",
    "            new_time_steps[1] = timesteps\n",
    "            y.set_shape(new_time_steps)\n",
    "        return y\n",
    "\n",
    "    def _generate_dropout_mask(self, inputs, training=None):\n",
    "        if 0 < self.dropout < 1:\n",
    "            ones = K.ones_like(K.squeeze(inputs[:, 0:1, :-1], axis=1))\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(ones, self.dropout)\n",
    "\n",
    "            self._dropout_mask = [K.in_train_phase(\n",
    "                dropped_inputs,\n",
    "                ones,\n",
    "                training=training)\n",
    "                for _ in range(3)]\n",
    "        else:\n",
    "            self._dropout_mask = None\n",
    "\n",
    "    def _generate_recurrent_dropout_mask(self, inputs, training=None):\n",
    "        if 0 < self.recurrent_dropout < 1:\n",
    "            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, self.units))\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(ones, self.dropout)\n",
    "\n",
    "            self._recurrent_dropout_mask = [K.in_train_phase(\n",
    "                dropped_inputs,\n",
    "                ones,\n",
    "                training=training)\n",
    "                for _ in range(3)]\n",
    "        else:\n",
    "            self._recurrent_dropout_mask = None\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # build an all-zero tensor of shape (samples, output_dim)\n",
    "        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n",
    "        initial_state = initial_state[:, :, :-1]\n",
    "        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n",
    "        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n",
    "        if hasattr(self.state_size, '__len__'):\n",
    "            return [K.tile(initial_state, [1, dim])\n",
    "                    for dim in self.state_size]\n",
    "        else:\n",
    "            return [K.tile(initial_state, [1, self.state_size])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
