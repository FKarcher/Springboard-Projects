{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import (Bidirectional, Dense)\n",
    "from attention_gru import SoftAttnGRU\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "class EpisodicMemoryModule(Layer):\n",
    "\n",
    "    def __init__(self, units,  emb_dim,\n",
    "                 batch_size, memory_steps=3, dropout=0.0, reuglarization=1e-3, **kwargs):\n",
    "        \"\"\"Initializes the Episodic Memory Module from\n",
    "         https://arxiv.org/pdf/1506.07285.pdf and https://arxiv.org/pdf/1603.01417.pdf.\n",
    "        The module has internally 2 dense layers used to compute attention,\n",
    "        one attention GRU unit, that modifies the layer input based on the computed attention,\n",
    "        and finally, one Dense layer that generates the new memory.\n",
    "        Have a look at the call method to get an idea of how everything works.\n",
    "        Parameters\n",
    "        ----------\n",
    "        units : (int)\n",
    "            The number of hidden units in the attention and memory networks\n",
    "        memory_steps : (int)\n",
    "            Number of steps to iterate over the input and generate the memory.\n",
    "        emb_dim : (int)\n",
    "            The size of the embeddings, and thus the number of units for the\n",
    "            attention computation\n",
    "        batch_size : (int)\n",
    "            Size of the batch\n",
    "        dropout : (float)\n",
    "            The dropout rate for the module\n",
    "        **kwargs : (arguments)\n",
    "            Extra arguments\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Dropout\n",
    "\n",
    "        self.memory_steps = memory_steps\n",
    "        self.dropout = dropout\n",
    "        self.name = \"episodic_memory_module\"\n",
    "        self._input_map = {}\n",
    "        self.supports_masking = True\n",
    "        self.units = units\n",
    "\n",
    "        # attention net.\n",
    "        self.l_1 = Dense(units=emb_dim,\n",
    "                         batch_size=batch_size,\n",
    "                         activation='tanh',\n",
    "                         kernel_regularizer=regularizers.l2(reuglarization))\n",
    "\n",
    "        self.l_2 = Dense(units=1,\n",
    "                         batch_size=batch_size,\n",
    "                         activation=None,\n",
    "                         kernel_regularizer=regularizers.l2(reuglarization))\n",
    "\n",
    "        # Episode net\n",
    "        self.episode_GRU = SoftAttnGRU(units=units,\n",
    "                                       return_sequences=False,\n",
    "                                       batch_size=batch_size,\n",
    "                                       kernel_regularizer=regularizers.l2(\n",
    "                                           0.001),\n",
    "                                       recurrent_regularizer=regularizers.l2(reuglarization))\n",
    "\n",
    "        # Memory generating net.\n",
    "        self.memory_net = Dense(units=units,\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=regularizers.l2(reuglarization))\n",
    "\n",
    "        super(EpisodicMemoryModule, self).__init__()\n",
    "\n",
    "    def get_config():\n",
    "        # TODO: Fix this to allow saving the entire model\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        q_shape = list(input_shape[1])\n",
    "        q_shape[-1] = self.units * 2\n",
    "\n",
    "        return tuple(q_shape)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(EpisodicMemoryModule, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Generates a new memory based on thequestion and\n",
    "        current inputs.\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : (list) of (K.Tensor)\n",
    "            A list of size two, where each element is a tensor. The first one is\n",
    "            the facts vector, and the second - the question vector.\n",
    "        Returns\n",
    "        -------\n",
    "        K.Tensor\n",
    "            A memory generated from the question and fact_vectors\n",
    "        \"\"\"\n",
    "\n",
    "        def compute_attention(fact, question, memory):\n",
    "            \"\"\"Computes an attention score over a single fact vector,\n",
    "            question and memoty\n",
    "            Parameters\n",
    "            ----------\n",
    "            fact : (K.tensor)\n",
    "                A single fact vector\n",
    "            question : (K.tensor)\n",
    "                Description of parameter `question`.\n",
    "            memory : (K.tensor)\n",
    "                The previous memory\n",
    "            Returns\n",
    "            -------\n",
    "            (K.tensor)\n",
    "                The scalar attention score for the current fact.\n",
    "            \"\"\"\n",
    "\n",
    "            f_i = [\n",
    "                fact * question,\n",
    "                fact * memory,\n",
    "                K.abs(\n",
    "                    fact - question),\n",
    "                K.abs(\n",
    "                    fact - memory)]\n",
    "            g_t_i = self.l_1(K.concatenate(f_i, axis=1))\n",
    "            g_t_i = self.l_2(g_t_i)\n",
    "            return g_t_i\n",
    "\n",
    "        facts = inputs[0]\n",
    "        question = inputs[1]\n",
    "        memory = K.identity(question)   # Initialize memory to the question\n",
    "        fact_list = tf.unstack(facts, axis=1)\n",
    "\n",
    "        for step in range(self.memory_steps):\n",
    "\n",
    "            # Adapted from\n",
    "            # https://github.com/barronalex/Dynamic-Memory-Networks-in-TensorFlow/\n",
    "\n",
    "            # Looks recurrent? In a way it is\n",
    "            attentions = [tf.squeeze(\n",
    "                compute_attention(fact, question, memory), axis=1)\n",
    "                for i, fact in enumerate(fact_list)]\n",
    "            print(\"Shape of attns:\", attentions.shape)\n",
    "            attentions = tf.stack(attentions)\n",
    "            attentions = tf.transpose(attentions)\n",
    "            attentions = tf.nn.softmax(attentions)\n",
    "            attentions = tf.expand_dims(attentions, axis=-1)\n",
    "\n",
    "            episode = K.concatenate([facts, attentions], axis=2)\n",
    "            # Last state. Correct? Maybe not\n",
    "            episode = self.episode_GRU(episode)\n",
    "\n",
    "            memory = self.memory_net(K.concatenate(\n",
    "                [memory, episode, question], axis=1))\n",
    "\n",
    "        return K.concatenate([memory, question], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
